<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd"
	version="0.10" xml:lang="en">
	<siteinfo>
		<sitename>Wikipedia</sitename>
		<dbname>enwiki</dbname>
		<base>https://en.wikipedia.org/wiki/Main_Page</base>
		<generator>MediaWiki 1.29.0-wmf.2</generator>
		<case>first-letter</case>
		<namespaces>
			<namespace key="-2" case="first-letter">Media</namespace>
			<namespace key="-1" case="first-letter">Special</namespace>
			<namespace key="0" case="first-letter" />
			<namespace key="1" case="first-letter">Talk</namespace>
			<namespace key="2" case="first-letter">User</namespace>
			<namespace key="3" case="first-letter">User talk</namespace>
			<namespace key="4" case="first-letter">Wikipedia</namespace>
			<namespace key="5" case="first-letter">Wikipedia talk</namespace>
			<namespace key="6" case="first-letter">File</namespace>
			<namespace key="7" case="first-letter">File talk</namespace>
			<namespace key="8" case="first-letter">MediaWiki</namespace>
			<namespace key="9" case="first-letter">MediaWiki talk</namespace>
			<namespace key="10" case="first-letter">Template</namespace>
			<namespace key="11" case="first-letter">Template talk</namespace>
			<namespace key="12" case="first-letter">Help</namespace>
			<namespace key="13" case="first-letter">Help talk</namespace>
			<namespace key="14" case="first-letter">Category</namespace>
			<namespace key="15" case="first-letter">Category talk</namespace>
			<namespace key="100" case="first-letter">Portal</namespace>
			<namespace key="101" case="first-letter">Portal talk</namespace>
			<namespace key="108" case="first-letter">Book</namespace>
			<namespace key="109" case="first-letter">Book talk</namespace>
			<namespace key="118" case="first-letter">Draft</namespace>
			<namespace key="119" case="first-letter">Draft talk</namespace>
			<namespace key="446" case="first-letter">Education Program</namespace>
			<namespace key="447" case="first-letter">Education Program talk</namespace>
			<namespace key="710" case="first-letter">TimedText</namespace>
			<namespace key="711" case="first-letter">TimedText talk</namespace>
			<namespace key="828" case="first-letter">Module</namespace>
			<namespace key="829" case="first-letter">Module talk</namespace>
			<namespace key="2300" case="first-letter">Gadget</namespace>
			<namespace key="2301" case="first-letter">Gadget talk</namespace>
			<namespace key="2302" case="case-sensitive">Gadget definition</namespace>
			<namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
		</namespaces>
	</siteinfo>
	<page>
		<title>Eigenvalue perturbation</title>
		<ns>0</ns>
		<id>10465001</id>
		<revision>
			<id>120342210</id>
			<timestamp>2007-04-04T22:13:04Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<comment>a start from my old notes</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="1873">{{expert}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving eigenvalues and eigenvectors of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the generalized eigenvalue problem,
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;.
We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term.

Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{0i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
Removing the &lt;math&gt;\delta^2&lt;/math&gt; terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M] + \delta \lambda_i [M_0]\mathbf{x}_{0i}&lt;/math&gt;.

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors.

In summary, we have
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i} \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

{{math-stub}}</text>
			<sha1>mnm70o3yaop6kqoustlxm88zwy29znb</sha1>
		</revision>
		<revision>
			<id>120349628</id>
			<parentid>120342210</parentid>
			<timestamp>2007-04-04T22:44:15Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<comment>/* Example */ a bit more</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="2072">{{expert}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving eigenvalues and eigenvectors of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the generalized eigenvalue problem,
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;.
We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term.

Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{0i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
Removing the &lt;math&gt;\delta^2&lt;/math&gt; terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M] + \delta \lambda_i [M_0]\mathbf{x}_{0i}&lt;/math&gt;.

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j}&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.

...


In summary, we have
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i} \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

{{math-stub}}</text>
			<sha1>9ykxxhx8w3j9urz9dsxqq29ocn5y16h</sha1>
		</revision>
		<revision>
			<id>120377740</id>
			<parentid>120349628</parentid>
			<timestamp>2007-04-05T01:05:01Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<comment>/* Example */ rest of details</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="2682">{{expert}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving eigenvalues and eigenvectors of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the generalized eigenvalue problem,
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;.
We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term.

Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{0i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
Removing the &lt;math&gt;\delta^2&lt;/math&gt; terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M] + \delta \lambda_i [M_0]\mathbf{x}_{0i} \qquad(1)&lt;/math&gt;.

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(2)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.

Substituting (2) into (1) and rearranging gives
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;
which provides the change in the ith eigenvalue as a function of changes in &lt;math&gt;[K]&lt;/math&gt; and &lt;math&gt;[M]&lt;/math&gt;.

...

Then
:&lt;math&gt;\epsilon{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \rdarrow \epsilon_{ii}=-\frac{1}{2}\mathnf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.


In summary, we have
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i} \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

{{math-stub}}</text>
			<sha1>7lwnzve78928saiqf60kx5kc31b7ptn</sha1>
		</revision>
		<revision>
			<id>120378997</id>
			<parentid>120377740</parentid>
			<timestamp>2007-04-05T01:11:42Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<comment>/* Example */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="2682">{{expert}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving eigenvalues and eigenvectors of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the generalized eigenvalue problem,
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;.
We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term.

Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{0i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
Removing the &lt;math&gt;\delta^2&lt;/math&gt; terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M] + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(1)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(2)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.

Substituting (2) into (1) and rearranging gives
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;
which provides the change in the ith eigenvalue as a function of changes in &lt;math&gt;[K]&lt;/math&gt; and &lt;math&gt;[M]&lt;/math&gt;.


Then
:&lt;math&gt;\epsilon{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.


In summary, we have
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i} \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

{{math-stub}}</text>
			<sha1>1o86kb2hp5lhotzgup9ed3yhwe1r3so</sha1>
		</revision>
		<revision>
			<id>120379212</id>
			<parentid>120378997</parentid>
			<timestamp>2007-04-05T01:12:52Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<minor />
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="2683">{{expert}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving eigenvalues and eigenvectors of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the generalized eigenvalue problem,
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;.
We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term.

Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{0i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
Removing the &lt;math&gt;\delta^2&lt;/math&gt; terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M] + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(1)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(2)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.

Substituting (2) into (1) and rearranging gives
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;
which provides the change in the ith eigenvalue as a function of changes in &lt;math&gt;[K]&lt;/math&gt; and &lt;math&gt;[M]&lt;/math&gt;.


Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.


In summary, we have
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i} \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

{{math-stub}}</text>
			<sha1>d6egw5ksd8vz12q2wmwhd792wvy4xv6</sha1>
		</revision>
		<revision>
			<id>120884420</id>
			<parentid>120379212</parentid>
			<timestamp>2007-04-07T02:28:58Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<comment>cat Perturbation theory</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="2717">{{expert}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving eigenvalues and eigenvectors of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the generalized eigenvalue problem,
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;.
We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term.

Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{0i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
Removing the &lt;math&gt;\delta^2&lt;/math&gt; terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M] + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(1)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(2)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.

Substituting (2) into (1) and rearranging gives
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;
which provides the change in the ith eigenvalue as a function of changes in &lt;math&gt;[K]&lt;/math&gt; and &lt;math&gt;[M]&lt;/math&gt;.


Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.


In summary, we have
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i} \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

{{math-stub}}

[[Category:Perturbation theory]]</text>
			<sha1>3xd1i7241a55rktm0r8dqasdy1zvc2q</sha1>
		</revision>
		<revision>
			<id>121133242</id>
			<parentid>120884420</parentid>
			<timestamp>2007-04-08T05:17:48Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<minor />
			<comment>links</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="2737">{{expert}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;.
We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term.

Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{0i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
Removing the &lt;math&gt;\delta^2&lt;/math&gt; terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M] + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(1)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(2)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.

Substituting (2) into (1) and rearranging gives
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;
which provides the change in the ith eigenvalue as a function of changes in &lt;math&gt;[K]&lt;/math&gt; and &lt;math&gt;[M]&lt;/math&gt;.


Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.


In summary, we have
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i} \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

{{math-stub}}

[[Category:Perturbation theory]]</text>
			<sha1>fc3nm4gio5f2uo5jylddgvny9ngd2un</sha1>
		</revision>
		<revision>
			<id>121133434</id>
			<parentid>121133242</parentid>
			<timestamp>2007-04-08T05:19:20Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<minor />
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="2738">{{expert}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.


Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{0i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
Removing the &lt;math&gt;\delta^2&lt;/math&gt; terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M] + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(1)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(2)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.

Substituting (2) into (1) and rearranging gives
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;
which provides the change in the ith eigenvalue as a function of changes in &lt;math&gt;[K]&lt;/math&gt; and &lt;math&gt;[M]&lt;/math&gt;.


Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.


In summary, we have
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i} \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

{{math-stub}}

[[Category:Perturbation theory]]</text>
			<sha1>8995mjny056w4afm326gcykwfdbcpds</sha1>
		</revision>
		<revision>
			<id>121133704</id>
			<parentid>121133434</parentid>
			<timestamp>2007-04-08T05:21:33Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<minor />
			<comment>/* Example */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="2831">{{expert}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{0i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
Removing the &lt;math&gt;\delta^2&lt;/math&gt; terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M] + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(1)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(2)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.

Substituting (2) into (1) and rearranging gives
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;
which provides the change in the ith eigenvalue as a function of changes in &lt;math&gt;[K]&lt;/math&gt; and &lt;math&gt;[M]&lt;/math&gt;.


Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.


In summary, we have
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i} \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

{{math-stub}}

[[Category:Perturbation theory]]</text>
			<sha1>f8tc6u9465lq6xqiy1r66hbpt8tcbu7</sha1>
		</revision>
		<revision>
			<id>121135498</id>
			<parentid>121133704</parentid>
			<timestamp>2007-04-08T05:35:13Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<comment>/* Example */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="3397">{{expert}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.

Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M] + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(1)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(2)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.

Substituting (2) into (1) and rearranging gives
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;
which provides the change in the ith eigenvalue as a function of changes in &lt;math&gt;[K]&lt;/math&gt; and &lt;math&gt;[M]&lt;/math&gt;.


Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.


In summary, we have
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i} \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

{{math-stub}}

[[Category:Perturbation theory]]</text>
			<sha1>lbll4d6yran71bq4qz0wyj3esmocfqg</sha1>
		</revision>
		<revision>
			<id>121137001</id>
			<parentid>121135498</parentid>
			<timestamp>2007-04-08T05:47:46Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<minor />
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="3970">{{expert}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i} \qquad (1)&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M] + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(2)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(3)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.

Substituting (3) into (2) and rearranging gives
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;
which provides the change in the ith eigenvalue as a function of changes in &lt;math&gt;[K]&lt;/math&gt; and &lt;math&gt;[M]&lt;/math&gt;.


Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.


In summary, we have
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i} \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

{{math-stub}}

[[Category:Perturbation theory]]</text>
			<sha1>si7y0w0ym78w2xpy5ebzl4yahwki684</sha1>
		</revision>
		<revision>
			<id>121137097</id>
			<parentid>121137001</parentid>
			<timestamp>2007-04-08T05:48:42Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<minor />
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="3923">{{expert}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i} \qquad (1)&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M] + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(2)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(3)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.

Substituting (3) into (2) and rearranging gives
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;
which provides the change in the ith eigenvalue as a function of changes in &lt;math&gt;[K]&lt;/math&gt; and &lt;math&gt;[M]&lt;/math&gt;.


Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.


In summary, we have
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i} \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

{{math-stub}}

[[Category:Perturbation theory]]</text>
			<sha1>70frocwuw6bxvzjnk0bg1vun8r8lyfp</sha1>
		</revision>
		<revision>
			<id>121137499</id>
			<parentid>121137097</parentid>
			<timestamp>2007-04-08T05:52:12Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<minor />
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="3938">{{expert}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i} \qquad (1)&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{io} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(2)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(3)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.

Substituting (3) into (2) and rearranging gives
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;
which provides the change in the ith eigenvalue as a function of changes in &lt;math&gt;[K]&lt;/math&gt; and &lt;math&gt;[M]&lt;/math&gt;.


Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.


In summary, we have
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i} \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

{{math-stub}}

[[Category:Perturbation theory]]</text>
			<sha1>j9o7mae60e20v3mnaivwh0mugj5lbzo</sha1>
		</revision>
		<revision>
			<id>121137511</id>
			<parentid>121137499</parentid>
			<timestamp>2007-04-08T05:52:15Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<minor />
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="3938">{{expert}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i} \qquad (1)&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(2)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(3)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.

Substituting (3) into (2) and rearranging gives
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;
which provides the change in the ith eigenvalue as a function of changes in &lt;math&gt;[K]&lt;/math&gt; and &lt;math&gt;[M]&lt;/math&gt;.


Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.


In summary, we have
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i} \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

{{math-stub}}

[[Category:Perturbation theory]]</text>
			<sha1>6h94pab88r0i5rqw3ui7kgamjrl9j14</sha1>
		</revision>
		<revision>
			<id>121144394</id>
			<parentid>121137511</parentid>
			<timestamp>2007-04-08T06:52:55Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<comment>more stuff</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="5051">{{expert}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i} \qquad (1)&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(2)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(3)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (3) into (2) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt;:
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T\lambda_{0i} [\delta M] \mathbf{x}_{0i} + \mathbf{x}_{0i}^T\delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Now the two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Thus, those terms can be be canceled leaving
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \mathbf{x}_{0i}^T\lambda_{0i}[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;
which provides the change in the ith eigenvalue as a function of changes in &lt;math&gt;[K]&lt;/math&gt; and &lt;math&gt;[M]&lt;/math&gt;.


Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.


In summary, we have
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i} \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

{{math-stub}}

[[Category:Perturbation theory]]</text>
			<sha1>pclfsr4huk9rwhs20nhucyh24kyne8r</sha1>
		</revision>
		<revision>
			<id>121146763</id>
			<parentid>121144394</parentid>
			<timestamp>2007-04-08T07:14:51Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<comment>more cleanup</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="5218">{{expert}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i} \qquad (1)&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(2)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(3)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (3) into (2) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (4)&lt;/math&gt;.
Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt;:
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(5) &lt;/math&gt;.
The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (5) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i}}&lt;/math&gt;
&lt;!--My notes have no fraction here (i.e., the denominator is zero not what it is above! --&gt;
which provides the change in the ith eigenvalue as a function of changes in &lt;math&gt;[K]&lt;/math&gt; and &lt;math&gt;[M]&lt;/math&gt;.

Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.


In summary, we have
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i} \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

{{math-stub}}

[[Category:Perturbation theory]]</text>
			<sha1>tnxiayusi9t67klsu2xac8gh0hhvgrg</sha1>
		</revision>
		<revision>
			<id>121146831</id>
			<parentid>121146763</parentid>
			<timestamp>2007-04-08T07:15:27Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<minor />
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="5213">{{expert}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i} \qquad (1)&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(2)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(3)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (3) into (2) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (4)&lt;/math&gt;.
Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt;:
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(5) &lt;/math&gt;.
The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (5) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;
&lt;!--My notes have no fraction here (i.e., the denominator is zero not what it is above! --&gt;
which provides the change in the ith eigenvalue as a function of changes in &lt;math&gt;[K]&lt;/math&gt; and &lt;math&gt;[M]&lt;/math&gt;.

Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.


In summary, we have
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i} \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

{{math-stub}}

[[Category:Perturbation theory]]</text>
			<sha1>tbmu9l3vkpw4zocok5wxyxmmbcq5aza</sha1>
		</revision>
		<revision>
			<id>121147103</id>
			<parentid>121146831</parentid>
			<timestamp>2007-04-08T07:17:51Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<comment>done for now</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="5219">{{expert}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i} \qquad (1)&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(2)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(3)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (3) into (2) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (4)&lt;/math&gt;.
Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt;:
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(5) &lt;/math&gt;.
The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (5) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;
['''fixme:'''My notes have no fraction here (i.e., the denominator is zero not what it is above!]
which provides the change in the ith eigenvalue as a function of changes in &lt;math&gt;[K]&lt;/math&gt; and &lt;math&gt;[M]&lt;/math&gt;.

Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.


In summary, we have
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i} \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

{{math-stub}}

[[Category:Perturbation theory]]</text>
			<sha1>hmkzymwr28qyewtdvmum7e7p60y6ztz</sha1>
		</revision>
		<revision>
			<id>121609460</id>
			<parentid>121147103</parentid>
			<timestamp>2007-04-10T04:58:27Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<comment>/* Example */ Figured it out.</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="5677">{{expert}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i} \qquad (1)&lt;/math&gt;.
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.


==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and that the eigenvectors are scaled such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} =\delta_i^j&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]]. Define &lt;math&gt;\omega_i&lt;/math&gt; as
:&lt;math&gt;\mathbf{x}_{0j}^T[K_0]\mathbf{x}_{0i} = \omega_i^2 \delta_i^j&lt;/math&gt;

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(2)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(3)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (3) into (2) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (4)&lt;/math&gt;.
Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt;:
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(5) &lt;/math&gt;.
The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (5) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But we have scaled the eigenvectors such that this denominator is equal to 1. Thus
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}&lt;/math&gt; ■

Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.


== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i} \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

{{math-stub}}

[[Category:Perturbation theory]]</text>
			<sha1>4si1ry2ushj4ggd1tdzpskvstifz01c</sha1>
		</revision>
		<revision>
			<id>121609587</id>
			<parentid>121609460</parentid>
			<timestamp>2007-04-10T04:59:21Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<minor />
			<comment>/* Steps */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="5692">{{expert}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i} \qquad (1)&lt;/math&gt;.
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.


==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and that the eigenvectors are scaled such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} =\delta_i^j&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]]. Define &lt;math&gt;\omega_i&lt;/math&gt; as
:&lt;math&gt;\mathbf{x}_{0j}^T[K_0]\mathbf{x}_{0i} = \omega_i^2 \delta_i^j&lt;/math&gt;

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(2)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(3)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (3) into (2) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (4)&lt;/math&gt;.
Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt;:
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(5) &lt;/math&gt;.
The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (5) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But we have scaled the eigenvectors such that this denominator is equal to 1. Thus
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i} \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

{{math-stub}}

[[Category:Perturbation theory]]</text>
			<sha1>3fxjqen46o0dqowwlp6reb52o7vsr5a</sha1>
		</revision>
		<revision>
			<id>121609650</id>
			<parentid>121609587</parentid>
			<timestamp>2007-04-10T04:59:50Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<minor />
			<comment>/* Summary */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="5694">{{expert}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i} \qquad (1)&lt;/math&gt;.
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.


==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and that the eigenvectors are scaled such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} =\delta_i^j&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]]. Define &lt;math&gt;\omega_i&lt;/math&gt; as
:&lt;math&gt;\mathbf{x}_{0j}^T[K_0]\mathbf{x}_{0i} = \omega_i^2 \delta_i^j&lt;/math&gt;

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(2)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(3)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (3) into (2) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (4)&lt;/math&gt;.
Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt;:
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(5) &lt;/math&gt;.
The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (5) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But we have scaled the eigenvectors such that this denominator is equal to 1. Thus
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

{{math-stub}}

[[Category:Perturbation theory]]</text>
			<sha1>86zfkrenbz96apikdiqtswmzua98hyq</sha1>
		</revision>
		<revision>
			<id>121653123</id>
			<parentid>121609650</parentid>
			<timestamp>2007-04-10T11:30:18Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<comment>/* Steps */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="5688">{{expert}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i} \qquad (1)&lt;/math&gt;.
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.


==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors are scaled such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]]. Define &lt;math&gt;\omega_i&lt;/math&gt; as
:&lt;math&gt;\mathbf{x}_{0j}^T[K_0]\mathbf{x}_{0i} = \omega_i^2 \delta_i^j&lt;/math&gt;

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.
Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt;:
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.
The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (6) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

{{math-stub}}

[[Category:Perturbation theory]]</text>
			<sha1>62iq61pquxzz6wsim6c8emc3k3klyd9</sha1>
		</revision>
		<revision>
			<id>121654487</id>
			<parentid>121653123</parentid>
			<timestamp>2007-04-10T11:42:22Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<comment>/* Summary */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="6345">{{expert}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i} \qquad (1)&lt;/math&gt;.
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.


==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors are scaled such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]]. Define &lt;math&gt;\omega_i&lt;/math&gt; as
:&lt;math&gt;\mathbf{x}_{0j}^T[K_0]\mathbf{x}_{0i} = \omega_i^2 \delta_i^j&lt;/math&gt;

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.
Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt;:
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.
The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (6) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices.
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
K_{(k\ell)}x_{0i(k)} x_{0i(\ell)}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i M_{(k\ell)} x_{0i(k)} x_{0i(\ell)}&lt;/math&gt;.

{{math-stub}}

[[Category:Perturbation theory]]</text>
			<sha1>4l4xfncxpfjkfg7uwhwy88hfs7nq46u</sha1>
		</revision>
		<revision>
			<id>121654728</id>
			<parentid>121654487</parentid>
			<timestamp>2007-04-10T11:44:30Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<minor />
			<comment>/* Results */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="6322">{{expert}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i} \qquad (1)&lt;/math&gt;.
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.


==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors are scaled such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]]. Define &lt;math&gt;\omega_i&lt;/math&gt; as
:&lt;math&gt;\mathbf{x}_{0j}^T[K_0]\mathbf{x}_{0i} = \omega_i^2 \delta_i^j&lt;/math&gt;

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.
Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt;:
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.
The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (6) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices.
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}&lt;/math&gt;.

{{math-stub}}

[[Category:Perturbation theory]]</text>
			<sha1>k8jmjjei0p37rpzstq98d35olrbzx9g</sha1>
		</revision>
		<revision>
			<id>121656077</id>
			<parentid>121654728</parentid>
			<timestamp>2007-04-10T11:56:31Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<comment>more results</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="6744">{{expert}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i} \qquad (1)&lt;/math&gt;.
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.


==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors are scaled such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]]. Define &lt;math&gt;\omega_i&lt;/math&gt; as
:&lt;math&gt;\mathbf{x}_{0j}^T[K_0]\mathbf{x}_{0i} = \omega_i^2 \delta_i^j&lt;/math&gt;

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.
Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt;:
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.
The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (6) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices.
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2} - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;.

{{math-stub}}

[[Category:Perturbation theory]]</text>
			<sha1>9zrhv2t5aybkv7ycrxogqij7enctv6b</sha1>
		</revision>
		<revision>
			<id>121741366</id>
			<parentid>121656077</parentid>
			<timestamp>2007-04-10T18:52:23Z</timestamp>
			<contributor>
				<ip>128.113.131.145</ip>
			</contributor>
			<comment>/* Results */ (2-\delta_k^\ell) stuff</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="7003">{{expert}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i} \qquad (1)&lt;/math&gt;.
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.


==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors are scaled such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]]. Define &lt;math&gt;\omega_i&lt;/math&gt; as
:&lt;math&gt;\mathbf{x}_{0j}^T[K_0]\mathbf{x}_{0i} = \omega_i^2 \delta_i^j&lt;/math&gt;

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.
Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt;:
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.
The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (6) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{{\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

{{math-stub}}

[[Category:Perturbation theory]]</text>
			<sha1>ixabbr005oeqov35dhwza7yrzar4zp3</sha1>
		</revision>
		<revision>
			<id>121741516</id>
			<parentid>121741366</parentid>
			<timestamp>2007-04-10T18:52:59Z</timestamp>
			<contributor>
				<ip>128.113.131.145</ip>
			</contributor>
			<comment>/* Results */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="7003">{{expert}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i} \qquad (1)&lt;/math&gt;.
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.


==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors are scaled such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]]. Define &lt;math&gt;\omega_i&lt;/math&gt; as
:&lt;math&gt;\mathbf{x}_{0j}^T[K_0]\mathbf{x}_{0i} = \omega_i^2 \delta_i^j&lt;/math&gt;

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.
Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt;:
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.
The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (6) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

{{math-stub}}

[[Category:Perturbation theory]]</text>
			<sha1>4xa72qqdaw3aja2jkpmx1ltwbruo4oy</sha1>
		</revision>
		<revision>
			<id>121741864</id>
			<parentid>121741516</parentid>
			<timestamp>2007-04-10T18:54:36Z</timestamp>
			<contributor>
				<ip>128.113.131.145</ip>
			</contributor>
			<comment>added {{dubious}} 'cause this needs some love from someone
				who knows more about or has a book on hand.</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="7015">{{expert}}
{{dubious}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i} \qquad (1)&lt;/math&gt;.
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.


==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors are scaled such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]]. Define &lt;math&gt;\omega_i&lt;/math&gt; as
:&lt;math&gt;\mathbf{x}_{0j}^T[K_0]\mathbf{x}_{0i} = \omega_i^2 \delta_i^j&lt;/math&gt;

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.
Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt;:
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.
The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (6) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

{{math-stub}}

[[Category:Perturbation theory]]</text>
			<sha1>023u1y4l8sq77sg76a78sjqtwtkilpy</sha1>
		</revision>
		<revision>
			<id>122135820</id>
			<parentid>121741864</parentid>
			<timestamp>2007-04-12T03:45:37Z</timestamp>
			<contributor>
				<username>Jmath666</username>
				<id>3550640</id>
			</contributor>
			<comment>needs reference to a reliable source</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="7030">{{expert}}
{{dubious}}
{{references}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i} \qquad (1)&lt;/math&gt;.
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.


==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors are scaled such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]]. Define &lt;math&gt;\omega_i&lt;/math&gt; as
:&lt;math&gt;\mathbf{x}_{0j}^T[K_0]\mathbf{x}_{0i} = \omega_i^2 \delta_i^j&lt;/math&gt;

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.
Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt;:
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.
The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (6) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

{{math-stub}}

[[Category:Perturbation theory]]</text>
			<sha1>hwt8fb2k0lrltfh1u0k8z1havhgfens</sha1>
		</revision>
		<revision>
			<id>122281350</id>
			<parentid>122135820</parentid>
			<timestamp>2007-04-12T18:29:45Z</timestamp>
			<contributor>
				<username>SmackBot</username>
				<id>433328</id>
			</contributor>
			<minor />
			<comment>Date/fix the maintenance tags</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="7031">{{expert}}{{dubious}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i} \qquad (1)&lt;/math&gt;.
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors are scaled such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]]. Define &lt;math&gt;\omega_i&lt;/math&gt; as
:&lt;math&gt;\mathbf{x}_{0j}^T[K_0]\mathbf{x}_{0i} = \omega_i^2 \delta_i^j&lt;/math&gt;

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.
Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt;:
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.
The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (6) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

[[Category:Perturbation theory]]</text>
			<sha1>lv09y45uthlcfwvgfjnrkbuadojexqx</sha1>
		</revision>
		<revision>
			<id>126866887</id>
			<parentid>122281350</parentid>
			<timestamp>2007-04-29T14:57:04Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<comment>rm extra tag</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="7020">{{expert}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i} \qquad (1)&lt;/math&gt;.
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors are scaled such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]]. Define &lt;math&gt;\omega_i&lt;/math&gt; as
:&lt;math&gt;\mathbf{x}_{0j}^T[K_0]\mathbf{x}_{0i} = \omega_i^2 \delta_i^j&lt;/math&gt;

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.
Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt;:
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.
The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (6) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

[[Category:Perturbation theory]]</text>
			<sha1>i502mxqqeqfer89e4o9jncdzn7f4gfs</sha1>
		</revision>
		<revision>
			<id>126866948</id>
			<parentid>126866887</parentid>
			<timestamp>2007-04-29T14:57:23Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<minor />
			<comment>/* Example */ mv period</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="7020">{{expert}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors are scaled such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]]. Define &lt;math&gt;\omega_i&lt;/math&gt; as
:&lt;math&gt;\mathbf{x}_{0j}^T[K_0]\mathbf{x}_{0i} = \omega_i^2 \delta_i^j&lt;/math&gt;

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.
Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt;:
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.
The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (6) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

[[Category:Perturbation theory]]</text>
			<sha1>izqbbozb2pwl7i1dtufj91mmgbuzh8t</sha1>
		</revision>
		<revision>
			<id>129661844</id>
			<parentid>126866948</parentid>
			<timestamp>2007-05-09T22:42:18Z</timestamp>
			<contributor>
				<username>Essap</username>
				<id>4295696</id>
			</contributor>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="7032">{{cleanup}}
{{expert}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors are scaled such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]]. Define &lt;math&gt;\omega_i&lt;/math&gt; as
:&lt;math&gt;\mathbf{x}_{0j}^T[K_0]\mathbf{x}_{0i} = \omega_i^2 \delta_i^j&lt;/math&gt;

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.
Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt;:
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.
The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (6) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

[[Category:Perturbation theory]]</text>
			<sha1>j4v672tjwq7b4j2t6bkjgvc1o0q9k46</sha1>
		</revision>
		<revision>
			<id>129790450</id>
			<parentid>129661844</parentid>
			<timestamp>2007-05-10T11:02:49Z</timestamp>
			<contributor>
				<username>SmackBot</username>
				<id>433328</id>
			</contributor>
			<minor />
			<comment>Date/fix the maintenance tags or gen fixes</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="7046">{{Cleanup|date=May 2007}}
{{expert}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors are scaled such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]]. Define &lt;math&gt;\omega_i&lt;/math&gt; as
:&lt;math&gt;\mathbf{x}_{0j}^T[K_0]\mathbf{x}_{0i} = \omega_i^2 \delta_i^j&lt;/math&gt;

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.
Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt;:
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.
The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (6) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

[[Category:Perturbation theory]]</text>
			<sha1>87oc0wx42fqbysp35c0g02twfsflitf</sha1>
		</revision>
		<revision>
			<id>175245425</id>
			<parentid>129790450</parentid>
			<timestamp>2007-12-02T11:09:26Z</timestamp>
			<contributor>
				<ip>81.108.234.28</ip>
			</contributor>
			<comment>/* Steps */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="7045">{{Cleanup|date=May 2007}}
{{expert}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors are scaled such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecka delta]]. Define &lt;math&gt;\omega_i&lt;/math&gt; as
:&lt;math&gt;\mathbf{x}_{0j}^T[K_0]\mathbf{x}_{0i} = \omega_i^2 \delta_i^j&lt;/math&gt;

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.
Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt;:
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.
The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (6) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

[[Category:Perturbation theory]]</text>
			<sha1>f9r16xvhw7qzdmbx4o5tdyufoxg51g4</sha1>
		</revision>
		<revision>
			<id>175245527</id>
			<parentid>175245425</parentid>
			<timestamp>2007-12-02T11:10:23Z</timestamp>
			<contributor>
				<ip>81.108.234.28</ip>
			</contributor>
			<comment>/* Steps */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="7046">{{Cleanup|date=May 2007}}
{{expert}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors are scaled such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]]. Define &lt;math&gt;\omega_i&lt;/math&gt; as
:&lt;math&gt;\mathbf{x}_{0j}^T[K_0]\mathbf{x}_{0i} = \omega_i^2 \delta_i^j&lt;/math&gt;

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.
Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt;:
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.
The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (6) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

[[Category:Perturbation theory]]</text>
			<sha1>87oc0wx42fqbysp35c0g02twfsflitf</sha1>
		</revision>
		<revision>
			<id>215345446</id>
			<parentid>175245527</parentid>
			<timestamp>2008-05-27T20:09:04Z</timestamp>
			<contributor>
				<ip>152.14.96.220</ip>
			</contributor>
			<comment>/* Steps */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="7035">{{Cleanup|date=May 2007}}
{{expert}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]]. Define &lt;math&gt;\omega_i&lt;/math&gt; as
:&lt;math&gt;\mathbf{x}_{0j}^T[K_0]\mathbf{x}_{0i} = \omega_i^2 \delta_i^j&lt;/math&gt;

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.
Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt;:
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.
The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (6) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

[[Category:Perturbation theory]]</text>
			<sha1>4aevkrjr93d95u59zv2ya8n8santzog</sha1>
		</revision>
		<revision>
			<id>227644985</id>
			<parentid>215345446</parentid>
			<timestamp>2008-07-24T15:39:27Z</timestamp>
			<contributor>
				<username>Bruguiea</username>
				<id>306480</id>
			</contributor>
			<minor />
			<comment>/* Steps */ adding a transpose sign</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="7037">{{Cleanup|date=May 2007}}
{{expert}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]]. Define &lt;math&gt;\omega_i&lt;/math&gt; as
:&lt;math&gt;\mathbf{x}_{0j}^T[K_0]\mathbf{x}_{0i} = \omega_i^2 \delta_i^j&lt;/math&gt;

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.
Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^T&lt;/math&gt;:
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.
The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (6) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

[[Category:Perturbation theory]]</text>
			<sha1>7ez0xfmyqt0gs59si93hyxe4ce6zglx</sha1>
		</revision>
		<revision>
			<id>227649124</id>
			<parentid>227644985</parentid>
			<timestamp>2008-07-24T16:02:18Z</timestamp>
			<contributor>
				<username>Bruguiea</username>
				<id>306480</id>
			</contributor>
			<comment>/* Steps */ They were missing steps in the proof</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="7896">{{Cleanup|date=May 2007}}
{{expert}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]]. Define &lt;math&gt;\omega_i&lt;/math&gt; as
:&lt;math&gt;\mathbf{x}_{0j}^T[K_0]\mathbf{x}_{0i} = \omega_i^2 \delta_i^j&lt;/math&gt;

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

By equation (1):
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^T&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^T \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} &lt;/math&gt;.

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (6) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

[[Category:Perturbation theory]]</text>
			<sha1>4bzesa8pv5joap9kmhpwm5xi2srayhu</sha1>
		</revision>
		<revision>
			<id>227649322</id>
			<parentid>227649124</parentid>
			<timestamp>2008-07-24T16:03:24Z</timestamp>
			<contributor>
				<username>Bruguiea</username>
				<id>306480</id>
			</contributor>
			<minor />
			<comment>/* Steps */ remove definition of omega, it is not used
				anywhere else in the article</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="7788">{{Cleanup|date=May 2007}}
{{expert}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

By equation (1):
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^T&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^T \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} &lt;/math&gt;.

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (6) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.
To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

[[Category:Perturbation theory]]</text>
			<sha1>9cjh3p8ytimmd9dwuw045uj0vdu57m7</sha1>
		</revision>
		<revision>
			<id>227660238</id>
			<parentid>227649322</parentid>
			<timestamp>2008-07-24T17:09:00Z</timestamp>
			<contributor>
				<username>Bruguiea</username>
				<id>306480</id>
			</contributor>
			<comment>/* Steps */ other mistakes</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="8078">{{Cleanup|date=May 2007}}
{{expert}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

By equation (1):
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^T&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^T \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} &lt;/math&gt;.

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^T&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (6) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):
:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k&lt;/math&gt;.

Or by changing the name of the indices:
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

[[Category:Perturbation theory]]</text>
			<sha1>n2aiatp8dpzhu8dnce80g2ktf4vtu09</sha1>
		</revision>
		<revision>
			<id>227663265</id>
			<parentid>227660238</parentid>
			<timestamp>2008-07-24T17:25:31Z</timestamp>
			<contributor>
				<username>Arthur Rubin</username>
				<id>374195</id>
			</contributor>
			<comment>Changed {{expert}} to {{expert-subject|Mathematics}},
				although it may be unnecessary</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="8098">{{Cleanup|date=May 2007}}
{{expert-subject|Mathematics}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to solving [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

By equation (1):
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^T&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^T \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} &lt;/math&gt;.

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^T&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (6) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):
:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k&lt;/math&gt;.

Or by changing the name of the indices:
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

[[Category:Perturbation theory]]</text>
			<sha1>dgrtmp66ipj81xqjl91rxs7kunnefio</sha1>
		</revision>
		<revision>
			<id>227663595</id>
			<parentid>227663265</parentid>
			<timestamp>2008-07-24T17:27:09Z</timestamp>
			<contributor>
				<username>Michael Hardy</username>
				<id>4626</id>
			</contributor>
			<minor />
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="8098">{{Cleanup|date=May 2007}}
{{expert-subject|Mathematics}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

By equation (1):
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^T&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^T \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} &lt;/math&gt;.

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^T&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (6) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):
:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k&lt;/math&gt;.

Or by changing the name of the indices:
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

[[Category:Perturbation theory]]</text>
			<sha1>81tmgh2vgxmhvtvs4pe61alqgpb0uvu</sha1>
		</revision>
		<revision>
			<id>227663841</id>
			<parentid>227663595</parentid>
			<timestamp>2008-07-24T17:28:34Z</timestamp>
			<contributor>
				<username>Arthur Rubin</username>
				<id>374195</id>
			</contributor>
			<comment>add category?</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="8126">{{Cleanup|date=May 2007}}
{{expert-subject|Mathematics}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;+ \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct
:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;
where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.
Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

By equation (1):
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^T&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^T \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} &lt;/math&gt;.

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^T&lt;/math&gt; gives
:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.
Canceling those terms in (6) leaves
:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.
Rearranging gives
:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus
:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):
:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k&lt;/math&gt;.

Or by changing the name of the indices:
:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j&lt;/math&gt;.

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use
:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}&lt;/math&gt;.

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>8maa7pm5zw3vdmh75u653nydbcirm19</sha1>
		</revision>
		<revision>
			<id>227663920</id>
			<parentid>227663841</parentid>
			<timestamp>2008-07-24T17:29:08Z</timestamp>
			<contributor>
				<username>Michael Hardy</username>
				<id>4626</id>
			</contributor>
			<minor />
			<comment>/* Steps */ probably adding some &quot;align&quot;
				environments in [[TeX]] would help here.</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="8144">{{Cleanup|date=May 2007}}
{{expert-subject|Mathematics}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}&lt;/math&gt;.

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

By equation (1):
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^T&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^T \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} &lt;/math&gt;.

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^T&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>ostqv7iwu4l2ijxlqqqb8enfvq4ujwy</sha1>
		</revision>
		<revision>
			<id>227664112</id>
			<parentid>227663920</parentid>
			<timestamp>2008-07-24T17:30:12Z</timestamp>
			<contributor>
				<username>Giftlite</username>
				<id>37986</id>
			</contributor>
			<minor />
			<comment>/* Example */ mv .</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="8144">{{Cleanup|date=May 2007}}
{{expert-subject|Mathematics}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}.&lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

By equation (1):
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^T&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^T \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} &lt;/math&gt;.

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^T&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>mtgzm63nt8mlhixpzerh6pnaey3i8tk</sha1>
		</revision>
		<revision>
			<id>249452283</id>
			<parentid>227664112</parentid>
			<timestamp>2008-11-03T18:23:02Z</timestamp>
			<contributor>
				<username>SmackBot</username>
				<id>433328</id>
			</contributor>
			<minor />
			<comment>Date maintenance tags and general fixes</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="8163">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}.&lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that
:&lt;math&gt;\mathbf{x}_{0j}^T[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

By equation (1):
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^T&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^T \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} &lt;/math&gt;.

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^T[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^T [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^T&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^T[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^T[M_0]\mathbf{x}_{0i}&lt;/math&gt;.

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^T[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^T[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^T [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^T[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^T_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^T_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^T_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^T_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^T_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^T_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^T_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>0wig4zuvm7dkt3k4efd4dzfshoybiw4</sha1>
		</revision>
		<revision>
			<id>304634615</id>
			<parentid>249452283</parentid>
			<timestamp>2009-07-28T07:34:52Z</timestamp>
			<contributor>
				<username>BenFrantzDale</username>
				<id>41799</id>
			</contributor>
			<minor />
			<comment>s/T/\top/</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="8253">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}.&lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that
:&lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

By equation (1):
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i} &lt;/math&gt;.

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}&lt;/math&gt;.

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>823bt7yxjaraxjjfzwgqmu22hdaa2np</sha1>
		</revision>
		<revision>
			<id>337096893</id>
			<parentid>304634615</parentid>
			<timestamp>2010-01-11T00:22:37Z</timestamp>
			<contributor>
				<username>Squatchmichael</username>
				<id>11272405</id>
			</contributor>
			<minor />
			<comment>Content edit: explained to viewers that the material was
				derived herein, and hence is self contained.</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="8403">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}.&lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that
:&lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

By equation (1):
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i} &lt;/math&gt;.

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}&lt;/math&gt;.

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>qlel11ivi6h31uvxqzewg3678gwaerr</sha1>
		</revision>
		<revision>
			<id>337098732</id>
			<parentid>337096893</parentid>
			<timestamp>2010-01-11T00:33:40Z</timestamp>
			<contributor>
				<username>Squatchmichael</username>
				<id>11272405</id>
			</contributor>
			<minor />
			<comment>Gave reference</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="8810">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}.&lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that
:&lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

By equation (1):
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i} &lt;/math&gt;.

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}&lt;/math&gt;.

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

==References==
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA
  isnb = 0898713617}}

*{{Cite book 
| first = Y. C. 
| last = Fung 
| publisher = Prentice-Hall, Inc.
| year = 1977 
| title = A First Course in Continuum Mechanics 
| edition = 2nd edition
| isbn = 0133183114}}

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>tf881sbnxwb4qy7ji3t2j85eu0ozm9u</sha1>
		</revision>
		<revision>
			<id>337098987</id>
			<parentid>337098732</parentid>
			<timestamp>2010-01-11T00:35:16Z</timestamp>
			<contributor>
				<username>Squatchmichael</username>
				<id>11272405</id>
			</contributor>
			<comment>Provided a reference</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="8617">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}.&lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that
:&lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

By equation (1):
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i} &lt;/math&gt;.

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}&lt;/math&gt;.

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

==References==
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isnb = 0898713617}}

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>3nl2fhgfb2zg5hxtzltrl655gqwp7d0</sha1>
		</revision>
		<revision>
			<id>337099620</id>
			<parentid>337098987</parentid>
			<timestamp>2010-01-11T00:39:30Z</timestamp>
			<contributor>
				<username>Squatchmichael</username>
				<id>11272405</id>
			</contributor>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="8900">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=Longman Group Limited (London) |isbn=0-582-44282-6 |page=83 |url=http://books.google.com/books?id=AJdfQL0rgrgC&amp;printsec=frontcover#v=onepage&amp;q=&amp;f=false}}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}.&lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that
:&lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

By equation (1):
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i} &lt;/math&gt;.

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}&lt;/math&gt;.

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

==References==
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isnb = 0898713617}}

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>dyalsi0tpqdoqnogaokw4x5y40rytfs</sha1>
		</revision>
		<revision>
			<id>337099896</id>
			<parentid>337099620</parentid>
			<timestamp>2010-01-11T00:41:20Z</timestamp>
			<contributor>
				<username>Squatchmichael</username>
				<id>11272405</id>
			</contributor>
			<minor />
			<comment>Provided source</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="8803">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}.&lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that
:&lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

By equation (1):
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i} &lt;/math&gt;.

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}&lt;/math&gt;.

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

==References==
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isnb = 0898713617}}

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>9cwttok7flnpy7pqcsefcpqozlwl9o4</sha1>
		</revision>
		<revision>
			<id>337100282</id>
			<parentid>337099896</parentid>
			<timestamp>2010-01-11T00:43:44Z</timestamp>
			<contributor>
				<username>Squatchmichael</username>
				<id>11272405</id>
			</contributor>
			<minor />
			<comment>included source</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="8817">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}.&lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that
:&lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

By equation (1):
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i} &lt;/math&gt;.

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}&lt;/math&gt;.

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

==References==
&lt;/references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isnb = 0898713617}}

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>6xtcdl08vixx0oj0s1c94lv2uo7f6b1</sha1>
		</revision>
		<revision>
			<id>337100356</id>
			<parentid>337100282</parentid>
			<timestamp>2010-01-11T00:44:19Z</timestamp>
			<contributor>
				<username>Squatchmichael</username>
				<id>11272405</id>
			</contributor>
			<minor />
			<comment>included source</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="8831">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}.&lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that
:&lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

By equation (1):
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i} &lt;/math&gt;.

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}&lt;/math&gt;.

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isnb = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>rfuxy8e8akqwsey1uyeya65ycl6fuof</sha1>
		</revision>
		<revision>
			<id>379695731</id>
			<parentid>337100356</parentid>
			<timestamp>2010-08-19T01:54:21Z</timestamp>
			<contributor>
				<username>Varlaam</username>
				<id>45645</id>
			</contributor>
			<minor />
			<comment>isbn</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="8831">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}.&lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that
:&lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

By equation (1):
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i} &lt;/math&gt;.

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}&lt;/math&gt;.

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>n92b8urgrr7egoq2b82gnxff8z2dk2w</sha1>
		</revision>
		<revision>
			<id>393302506</id>
			<parentid>379695731</parentid>
			<timestamp>2010-10-27T22:14:14Z</timestamp>
			<contributor>
				<username>Acx01b</username>
				<id>10840236</id>
			</contributor>
			<comment>/* Summary */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="8948">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}.&lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that
:&lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

By equation (1):
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i} &lt;/math&gt;.

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}&lt;/math&gt;.

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinetesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being neglibible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>flr3690krc5h24tircilh8iguzbhvqr</sha1>
		</revision>
		<revision>
			<id>393460479</id>
			<parentid>393302506</parentid>
			<timestamp>2010-10-28T17:32:15Z</timestamp>
			<contributor>
				<username>Acx01b</username>
				<id>10840236</id>
			</contributor>
			<comment>/* Summary */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="8948">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

'''Eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],
:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;
That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.
Now suppose we want to change the matrices by a small amount. That is, we want to let
:&lt;math&gt;[K] = [K_0]+[\delta K]&lt;/math&gt;
and
:&lt;math&gt;[M] = [M_0]+[\delta M]&lt;/math&gt;
where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form
:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}+\delta\mathbf{x}_{0i}.&lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that
:&lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;
where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation
:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i&lt;/math&gt;.
Substituting, we get
:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i})&lt;/math&gt;.
which expands to
:&lt;math&gt;[K_0]\mathbf{x}_{0i}+[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Canceling from (1) leaves
:&lt;math&gt;[\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i &lt;/math&gt;
:::&lt;math&gt;=   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i}  &lt;/math&gt;
::::&lt;math&gt;{} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i &lt;/math&gt;.
Removing the higher-order terms, this simplifies to
:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

We note that, when the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives
:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i} \qquad (5)&lt;/math&gt;.

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

By equation (1):
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i} &lt;/math&gt;.

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i} ~~(6) &lt;/math&gt;.

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}&lt;/math&gt;.

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}&lt;/math&gt;.

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinetesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)
:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2-\delta_k^\ell)&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell)&lt;/math&gt;.
Similarly
:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;
and
:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell)&lt;/math&gt;.

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>6ujewf1aufwfvo1tyopju9ap75quv4w</sha1>
		</revision>
		<revision>
			<id>398523323</id>
			<parentid>393460479</parentid>
			<timestamp>2010-11-23T22:29:49Z</timestamp>
			<contributor>
				<username>Michael Hardy</username>
				<id>4626</id>
			</contributor>
			<comment>format</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9026">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
[K_0]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
[\delta K]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinetesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>f7til329blxln38121p0k3iq5cycdxe</sha1>
		</revision>
		<revision>
			<id>415711346</id>
			<parentid>398523323</parentid>
			<timestamp>2011-02-24T16:43:05Z</timestamp>
			<contributor>
				<username>Riemannzeta81</username>
				<id>3157447</id>
			</contributor>
			<comment>The proof displayed in the page is wrong and it leads to
				erroneous conclusions.</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="1816">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>clzvlvrjr0s2webatlpld57epgqqwpa</sha1>
		</revision>
		<revision>
			<id>415714833</id>
			<parentid>415711346</parentid>
			<timestamp>2011-02-24T17:07:33Z</timestamp>
			<contributor>
				<username>Bility</username>
				<id>7593235</id>
			</contributor>
			<minor />
			<comment>Undid revision 415711346 by
				[[Special:Contributions/Riemannzeta81|Riemannzeta81]] ([[User
				talk:Riemannzeta81|talk]]) unblanking</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9026">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
[K_0]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
[\delta K]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinetesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>f7til329blxln38121p0k3iq5cycdxe</sha1>
		</revision>
		<revision>
			<id>416488811</id>
			<parentid>415714833</parentid>
			<timestamp>2011-03-01T02:12:54Z</timestamp>
			<contributor>
				<ip>219.134.224.132</ip>
			</contributor>
			<comment>/* Example */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="8306">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
[K_0]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
[\delta K]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinetesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>ntju5vesh1ovv530h4rfafh5cdsj8l1</sha1>
		</revision>
		<revision>
			<id>416488883</id>
			<parentid>416488811</parentid>
			<timestamp>2011-03-01T02:13:23Z</timestamp>
			<contributor>
				<ip>219.134.224.132</ip>
			</contributor>
			<comment>/* Summary */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="7795">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
[K_0]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
[\delta K]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>9ayscgwj3udium0cmxh78slgb44pil8</sha1>
		</revision>
		<revision>
			<id>416488931</id>
			<parentid>416488883</parentid>
			<timestamp>2011-03-01T02:13:43Z</timestamp>
			<contributor>
				<ip>219.134.224.132</ip>
			</contributor>
			<comment>/* Results */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="6464">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
[K_0]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
[\delta K]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>adsd5096db4kns4sbl4rjaa0g92klo5</sha1>
		</revision>
		<revision>
			<id>416501444</id>
			<parentid>416488931</parentid>
			<timestamp>2011-03-01T03:50:38Z</timestamp>
			<contributor>
				<username>Arthur Rubin</username>
				<id>374195</id>
			</contributor>
			<minor />
			<comment>[[Help:Reverting|Reverted]] edits by
				[[Special:Contributions/219.134.224.132|219.134.224.132]] ([[User
				talk:219.134.224.132|talk]]) to last version by Bility</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9026">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
[K_0]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
[\delta K]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinetesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>f7til329blxln38121p0k3iq5cycdxe</sha1>
		</revision>
		<revision>
			<id>416504067</id>
			<parentid>416501444</parentid>
			<timestamp>2011-03-01T04:13:38Z</timestamp>
			<contributor>
				<username>Jiangyi614</username>
				<id>14098977</id>
			</contributor>
			<comment>/* Steps */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="3658">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinetesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>d4anddtji1wqn98aityorzoc9wr0bmk</sha1>
		</revision>
		<revision>
			<id>416504122</id>
			<parentid>416504067</parentid>
			<timestamp>2011-03-01T04:14:11Z</timestamp>
			<contributor>
				<username>Jiangyi614</username>
				<id>14098977</id>
			</contributor>
			<comment>/* Summary */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="3147">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>oevekuypnpaei1qwj8r0mrr5ac9h4ju</sha1>
		</revision>
		<revision>
			<id>416504125</id>
			<parentid>416504122</parentid>
			<timestamp>2011-03-01T04:14:12Z</timestamp>
			<contributor>
				<username>HalfShadow</username>
				<id>492858</id>
			</contributor>
			<minor />
			<comment>[[Help:Reverting|Reverted]] edits by
				[[Special:Contributions/Jiangyi614|Jiangyi614]] ([[User
				talk:Jiangyi614|talk]]) to last version by Arthur Rubin</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9026">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
[K_0]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
[\delta K]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinetesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>f7til329blxln38121p0k3iq5cycdxe</sha1>
		</revision>
		<revision>
			<id>416504161</id>
			<parentid>416504125</parentid>
			<timestamp>2011-03-01T04:14:37Z</timestamp>
			<contributor>
				<username>Jiangyi614</username>
				<id>14098977</id>
			</contributor>
			<comment>/* Summary */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="8515">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
[K_0]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
[\delta K]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>huo07lyh52lhw0x88ub5atols0giycp</sha1>
		</revision>
		<revision>
			<id>416504190</id>
			<parentid>416504161</parentid>
			<timestamp>2011-03-01T04:14:52Z</timestamp>
			<contributor>
				<username>Bped1985</username>
				<id>13580304</id>
			</contributor>
			<minor />
			<comment>Reverted edits by
				[[Special:Contributions/Jiangyi614|Jiangyi614]] ([[User
				talk:Jiangyi614|talk]]) unexplained removal of content
				([[WP:HG|HG]])</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9026">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
[K_0]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
[\delta K]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinetesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>f7til329blxln38121p0k3iq5cycdxe</sha1>
		</revision>
		<revision>
			<id>416504472</id>
			<parentid>416504190</parentid>
			<timestamp>2011-03-01T04:17:08Z</timestamp>
			<contributor>
				<username>Jiangyi614</username>
				<id>14098977</id>
			</contributor>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="1095">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.
==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>h4q0fu3wgl3rykgj66r9npvg82as1vi</sha1>
		</revision>
		<revision>
			<id>416504504</id>
			<parentid>416504472</parentid>
			<timestamp>2011-03-01T04:17:25Z</timestamp>
			<contributor>
				<username>Bped1985</username>
				<id>13580304</id>
			</contributor>
			<minor />
			<comment>Reverted edits by
				[[Special:Contributions/Jiangyi614|Jiangyi614]] ([[User
				talk:Jiangyi614|talk]]) unexplained removal of content
				([[WP:HG|HG]])</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9026">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
[K_0]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
[\delta K]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinetesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>f7til329blxln38121p0k3iq5cycdxe</sha1>
		</revision>
		<revision>
			<id>416506893</id>
			<parentid>416504504</parentid>
			<timestamp>2011-03-01T04:39:50Z</timestamp>
			<contributor>
				<username>Jiangyi614</username>
				<id>14098977</id>
			</contributor>
			<comment>/* Example */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9018">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;A \mathbf{x}_{0i} = \lambda_{0i} B \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
[K_0]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
[\delta K]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinetesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>s5pbzimu8adhr9vt2ywyi0vh4tucnfr</sha1>
		</revision>
		<revision>
			<id>416506983</id>
			<parentid>416506893</parentid>
			<timestamp>2011-03-01T04:40:46Z</timestamp>
			<contributor>
				<username>Jiangyi614</username>
				<id>14098977</id>
			</contributor>
			<comment>/* Example */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9015">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;A \mathbf{x}_{i} = \lambda_{i} B \mathbf{x}_{i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
[K_0]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
[\delta K]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinetesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>gjv2yv9nifuqg28kzph2n3ssolu52t0</sha1>
		</revision>
		<revision>
			<id>416507100</id>
			<parentid>416506983</parentid>
			<timestamp>2011-03-01T04:41:44Z</timestamp>
			<contributor>
				<username>HalfShadow</username>
				<id>492858</id>
			</contributor>
			<minor />
			<comment>[[Help:Reverting|Reverted]] edits by
				[[Special:Contributions/Jiangyi614|Jiangyi614]] ([[User
				talk:Jiangyi614|talk]]) to last version by Bped1985</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9026">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
[K_0]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
[\delta K]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinetesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>f7til329blxln38121p0k3iq5cycdxe</sha1>
		</revision>
		<revision>
			<id>416507818</id>
			<parentid>416507100</parentid>
			<timestamp>2011-03-01T04:48:36Z</timestamp>
			<contributor>
				<username>Jiangyi19850614</username>
				<id>14099097</id>
			</contributor>
			<comment>/* Example */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9018">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;A \mathbf{x}_{i} = \lambda_{0i} [B] \mathbf{x}_{i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
[K_0]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
[\delta K]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinetesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>iwupxfd1iwpmrgi6u689nclhzqalwfa</sha1>
		</revision>
		<revision>
			<id>416507915</id>
			<parentid>416507818</parentid>
			<timestamp>2011-03-01T04:49:16Z</timestamp>
			<contributor>
				<username>Jiangyi19850614</username>
				<id>14099097</id>
			</contributor>
			<comment>/* Example */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9016">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;A \mathbf{x}_{i} = \lambda_{0i} B \mathbf{x}_{i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
[K_0]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
[\delta K]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinetesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>5lts2sagdtark80kmb1yhknxpd0u5g6</sha1>
		</revision>
		<revision>
			<id>416508113</id>
			<parentid>416507915</parentid>
			<timestamp>2011-03-01T04:51:04Z</timestamp>
			<contributor>
				<username>HalfShadow</username>
				<id>492858</id>
			</contributor>
			<minor />
			<comment>[[Help:Reverting|Reverted]] edits by
				[[Special:Contributions/Jiangyi19850614|Jiangyi19850614]] ([[User
				talk:Jiangyi19850614|talk]]) to last version by HalfShadow</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9026">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
[K_0]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
[\delta K]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinetesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>f7til329blxln38121p0k3iq5cycdxe</sha1>
		</revision>
		<revision>
			<id>418858082</id>
			<parentid>416508113</parentid>
			<timestamp>2011-03-14T22:46:01Z</timestamp>
			<contributor>
				<ip>171.66.48.188</ip>
			</contributor>
			<comment>Fixed a couple LaTeX typos that hid critical terms in the
				derivation</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9048">{{Cleanup|date=May 2007}}
{{Expert-subject|Mathematics|date=November 2008}}
{{Unreferenced|date=April 2007}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinetesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>48xs5whymcybqhm836ohz1fem3wto0a</sha1>
		</revision>
		<revision>
			<id>463683442</id>
			<parentid>418858082</parentid>
			<timestamp>2011-12-02T16:42:28Z</timestamp>
			<contributor>
				<username>AvicBot</username>
				<id>11952314</id>
			</contributor>
			<minor />
			<comment>Bot: Grouping Multiple Issues together ([[User
				talk:Avicennasis|Report error]])</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9039">{{Multiple issues
|unreferenced = April 2007
|expert-subject = November 2008
|cleanup = May 2007
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinetesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>5n7y60ksolbvq8408390lm6y24wc25m</sha1>
		</revision>
		<revision>
			<id>465038304</id>
			<parentid>463683442</parentid>
			<timestamp>2011-12-09T23:38:25Z</timestamp>
			<contributor>
				<username>Sodin</username>
				<id>251178</id>
			</contributor>
			<comment>/* Steps */ disambig</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9064">{{Multiple issues
|unreferenced = April 2007
|expert-subject = November 2008
|cleanup = May 2007
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinetesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>jk8eborikh18vx5eyrgauq7w4jkoa5o</sha1>
		</revision>
		<revision>
			<id>466442995</id>
			<parentid>465038304</parentid>
			<timestamp>2011-12-18T02:02:39Z</timestamp>
			<contributor>
				<username>AvicAWB</username>
				<id>12259261</id>
			</contributor>
			<minor />
			<comment>clean up using [[Project:AWB|AWB]]</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9062">{{Multiple issues
|refimprove = April 2007
|expert-subject = November 2008
|cleanup = May 2007
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinetesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0898713617}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>6rivtguaxrj3trbfd905tk6buu08q5y</sha1>
		</revision>
		<revision>
			<id>492058806</id>
			<parentid>466442995</parentid>
			<timestamp>2012-05-11T19:23:09Z</timestamp>
			<contributor>
				<username>Helpful Pixie Bot</username>
				<id>14216826</id>
			</contributor>
			<minor />
			<comment>ISBNs (Build KE)</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9065">{{Multiple issues
|refimprove = April 2007
|expert-subject = November 2008
|cleanup = May 2007
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinetesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>bdikc3iuxwuq5m474ajrlyuoux6fk84</sha1>
		</revision>
		<revision>
			<id>492666470</id>
			<parentid>492058806</parentid>
			<timestamp>2012-05-15T09:32:59Z</timestamp>
			<contributor>
				<username>Fram</username>
				<id>390477</id>
			</contributor>
			<minor />
			<comment>Correct multiple issues template and general fixes,
				replaced: |expert-subject → |expert using [[Project:AWB|AWB]] (7916)</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9057">{{Multiple issues
|refimprove = April 2007
|expert = November 2008
|cleanup = May 2007
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinetesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>1fdmi2erc6cnixyw6kzfxurjd62ramc</sha1>
		</revision>
		<revision>
			<id>502479122</id>
			<parentid>492666470</parentid>
			<timestamp>2012-07-15T19:03:32Z</timestamp>
			<contributor>
				<username>Michael Hardy</username>
				<id>4626</id>
			</contributor>
			<comment>/* Steps */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9063">{{Multiple issues
|refimprove = April 2007
|expert = November 2008
|cleanup = May 2007
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left-multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinetesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>jnc3q8ptujytcha7x6nxw0jupfo0rus</sha1>
		</revision>
		<revision>
			<id>502479249</id>
			<parentid>502479122</parentid>
			<timestamp>2012-07-15T19:04:29Z</timestamp>
			<contributor>
				<username>Michael Hardy</username>
				<id>4626</id>
			</contributor>
			<comment>authorlink</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9093">{{Multiple issues
|refimprove = April 2007
|expert = November 2008
|cleanup = May 2007
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |authorlink=Lloyd N. Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left-multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinetesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>1onq93ujy4vza1l3j7h5biim7lmpaz8</sha1>
		</revision>
		<revision>
			<id>502479508</id>
			<parentid>502479249</parentid>
			<timestamp>2012-07-15T19:06:09Z</timestamp>
			<contributor>
				<username>Michael Hardy</username>
				<id>4626</id>
			</contributor>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9099">{{Multiple issues
|refimprove = April 2007
|expert = November 2008
|cleanup = May 2007
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |authorlink=Lloyd Nicholas Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left-multiplying equation (6) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0k}[\delta M])\mathbf{x}_{0k}}{\lambda_{0k}-\lambda_{0i}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0j}[\delta M])\mathbf{x}_{0j}}{\lambda_{0j}-\lambda_{0i}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinetesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>1i1qi1lojti0cspzvluukkh6fo7tjyu</sha1>
		</revision>
		<revision>
			<id>519959268</id>
			<parentid>502479508</parentid>
			<timestamp>2012-10-26T15:06:59Z</timestamp>
			<contributor>
				<ip>82.21.97.145</ip>
			</contributor>
			<comment>/* Steps */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9099">{{Multiple issues
|refimprove = April 2007
|expert = November 2008
|cleanup = May 2007
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |authorlink=Lloyd Nicholas Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left-multiplying equation (5) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0k}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinetesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>gngxnnkncdwesg7z8jwn9euyzpjo3ib</sha1>
		</revision>
		<revision>
			<id>522624900</id>
			<parentid>519959268</parentid>
			<timestamp>2012-11-12T12:04:03Z</timestamp>
			<contributor>
				<ip>194.176.105.138</ip>
			</contributor>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9218">{{Multiple issues
|refimprove = April 2007
|expert = November 2008
|cleanup = May 2007
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |authorlink=Lloyd Nicholas Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left-multiplying equation (5) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0k}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinetesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;
==Related Topics==

Bounds exist that do not rely on approximations in the [[Bauer–Fike_theorem|Bauer-Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>4tna5mqartc81yewe1aef2ohuevdwuo</sha1>
		</revision>
		<revision>
			<id>530059013</id>
			<parentid>522624900</parentid>
			<timestamp>2012-12-27T23:56:53Z</timestamp>
			<contributor>
				<ip>108.77.139.10</ip>
			</contributor>
			<comment>/* Summary */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9218">{{Multiple issues
|refimprove = April 2007
|expert = November 2008
|cleanup = May 2007
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |authorlink=Lloyd Nicholas Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left-multiplying equation (5) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0k}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinitesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;
==Related Topics==

Bounds exist that do not rely on approximations in the [[Bauer–Fike_theorem|Bauer-Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>egje3k1frpvhhynpfbanpx5odom6cfa</sha1>
		</revision>
		<revision>
			<id>532271592</id>
			<parentid>530059013</parentid>
			<timestamp>2013-01-09T23:41:49Z</timestamp>
			<contributor>
				<username>WQUlrich</username>
				<id>9644896</id>
			</contributor>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9197">{{Multiple issues
|refimprove = April 2007
|expert = November 2008}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |authorlink=Lloyd Nicholas Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left-multiplying equation (5) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0k}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinitesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;
==Related Topics==

Bounds exist that do not rely on approximations in the [[Bauer–Fike_theorem|Bauer-Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=L.N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>8yau8a33oux04dpwe45gjmnycxu9zsp</sha1>
		</revision>
		<revision>
			<id>537919395</id>
			<parentid>532271592</parentid>
			<timestamp>2013-02-12T19:34:42Z</timestamp>
			<contributor>
				<username>GirasoleDE</username>
				<id>3961223</id>
			</contributor>
			<minor />
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9195">{{Multiple issues
|refimprove = April 2007
|expert = November 2008}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |authorlink=Lloyd N. Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left-multiplying equation (5) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0k}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinitesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;
==Related Topics==

Bounds exist that do not rely on approximations in the [[Bauer–Fike_theorem|Bauer-Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=Lloyd N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>l0c6olpfp7bsms80vdm8l1rh27bvfs8</sha1>
		</revision>
		<revision>
			<id>542397516</id>
			<parentid>537919395</parentid>
			<timestamp>2013-03-06T17:12:49Z</timestamp>
			<contributor>
				<username>BattyBot</username>
				<id>15996738</id>
			</contributor>
			<comment>Converted {{Multiple issues}} to new format to fix expert
				parameter &amp; [[WP:AWB/GF|general fixes]] using
				[[Project:AWB|AWB]] (8853)</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9212">{{Multiple issues|
{{refimprove|date=April 2007}}
{{expert-subject|date=November 2008}}
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |authorlink=Lloyd N. Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left-multiplying equation (5) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0k}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinitesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==See also==

Bounds exist that do not rely on approximations in the [[Bauer–Fike theorem|Bauer-Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=Lloyd N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>3j56zygsvdt9cm9s6vrelboiwrzogh2</sha1>
		</revision>
		<revision>
			<id>553297981</id>
			<parentid>542397516</parentid>
			<timestamp>2013-05-03T03:57:36Z</timestamp>
			<contributor>
				<ip>99.235.250.152</ip>
			</contributor>
			<comment>/* References */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9311">{{Multiple issues|
{{refimprove|date=April 2007}}
{{expert-subject|date=November 2008}}
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |authorlink=Lloyd N. Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left-multiplying equation (5) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0k}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinitesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==See also==

Bounds exist that do not rely on approximations in the [[Bauer–Fike theorem|Bauer-Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=Lloyd N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}
*[http://terrytao.wordpress.com/2008/10/28/when-are-eigenvalues-stable/ Prof. Terrence Tao's blog]

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>4f48fq5rgak4215kqon10qbe9p1p7qv</sha1>
		</revision>
		<revision>
			<id>553298053</id>
			<parentid>553297981</parentid>
			<timestamp>2013-05-03T03:58:39Z</timestamp>
			<contributor>
				<ip>99.235.250.152</ip>
			</contributor>
			<comment>Undid revision 553297981 by
				[[Special:Contributions/99.235.250.152|99.235.250.152]] ([[User
				talk:99.235.250.152|talk]])</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9212">{{Multiple issues|
{{refimprove|date=April 2007}}
{{expert-subject|date=November 2008}}
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |authorlink=Lloyd N. Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left-multiplying equation (5) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0k}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinitesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==See also==

Bounds exist that do not rely on approximations in the [[Bauer–Fike theorem|Bauer-Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=Lloyd N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>3j56zygsvdt9cm9s6vrelboiwrzogh2</sha1>
		</revision>
		<revision>
			<id>553298200</id>
			<parentid>553298053</parentid>
			<timestamp>2013-05-03T04:00:17Z</timestamp>
			<contributor>
				<ip>99.235.250.152</ip>
			</contributor>
			<comment>Added a link to Prof. Tao's blog at UCLA.</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9331">{{Multiple issues|
{{refimprove|date=April 2007}}
{{expert-subject|date=November 2008}}
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |authorlink=Lloyd N. Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left-multiplying equation (5) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0k}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinitesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==See also==

Bounds exist that do not rely on approximations in the [[Bauer–Fike theorem|Bauer-Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=Lloyd N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

==External links==
*[http://terrytao.wordpress.com/2008/10/28/when-are-eigenvalues-stable/ Prof. Terrence Tao's blog]

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>mck4d593jlrqondv1wcwjhvpvqk2wzn</sha1>
		</revision>
		<revision>
			<id>553298307</id>
			<parentid>553298200</parentid>
			<timestamp>2013-05-03T04:01:02Z</timestamp>
			<contributor>
				<ip>99.235.250.152</ip>
			</contributor>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9330">{{Multiple issues|
{{refimprove|date=April 2007}}
{{expert-subject|date=November 2008}}
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |authorlink=Lloyd N. Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left-multiplying equation (5) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0k}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinitesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==See also==

Bounds exist that do not rely on approximations in the [[Bauer–Fike theorem|Bauer-Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=Lloyd N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

==External links==
[http://terrytao.wordpress.com/2008/10/28/when-are-eigenvalues-stable/ Prof. Terrence Tao's blog]

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>ayvulq2a2o97or4gn02oco7857dcwh0</sha1>
		</revision>
		<revision>
			<id>553298537</id>
			<parentid>553298307</parentid>
			<timestamp>2013-05-03T04:03:05Z</timestamp>
			<contributor>
				<ip>99.235.250.152</ip>
			</contributor>
			<comment>Added a link to Prof. Tao's blog at UCLA. My previous
				attempts to add this didn't work</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9539">{{Multiple issues|
{{refimprove|date=April 2007}}
{{expert-subject|date=November 2008}}
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |authorlink=Lloyd N. Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left-multiplying equation (5) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0k}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinitesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==See also==

See Prof. Terrence Tao's blog post [http://==External%20links==%20*%5Bhttp://terrytao.wordpress.com/2008/10/28/when-are-eigenvalues-stable/%20Prof.%20Terrence%20Tao's%20blog%5D When are eigenvalues stable].


Bounds exist that do not rely on approximations in the [[Bauer–Fike theorem|Bauer-Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=Lloyd N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

==External links==
[http://terrytao.wordpress.com/2008/10/28/when-are-eigenvalues-stable/ Prof. Terrence Tao's blog]

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>oqc507qwj6qxu9l7nqt56hrw6qa3psl</sha1>
		</revision>
		<revision>
			<id>553298629</id>
			<parentid>553298537</parentid>
			<timestamp>2013-05-03T04:04:22Z</timestamp>
			<contributor>
				<username>XLinkBot</username>
				<id>6163802</id>
			</contributor>
			<comment>BOT--Reverting link addition(s) by
				[[:en:Special:Contributions/99.235.250.152|99.235.250.152]] to
				revision 542397516
				(http://==External%20links==%20*[http://terrytao.wordpress.com/2008/10/28/when-are-eigenvalues-stable/%20Prof.%20Terrence%20Tao's%20blo...</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9212">{{Multiple issues|
{{refimprove|date=April 2007}}
{{expert-subject|date=November 2008}}
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |authorlink=Lloyd N. Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_i) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left-multiplying equation (5) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0k}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinitesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==See also==

Bounds exist that do not rely on approximations in the [[Bauer–Fike theorem|Bauer-Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=Lloyd N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>3j56zygsvdt9cm9s6vrelboiwrzogh2</sha1>
		</revision>
		<revision>
			<id>574624282</id>
			<parentid>553298629</parentid>
			<timestamp>2013-09-26T16:32:26Z</timestamp>
			<contributor>
				<ip>140.180.248.106</ip>
			</contributor>
			<comment>/* Steps */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9215">{{Multiple issues|
{{refimprove|date=April 2007}}
{{expert-subject|date=November 2008}}
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |authorlink=Lloyd N. Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_{0i}) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{0i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left-multiplying equation (5) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0k}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinitesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==See also==

Bounds exist that do not rely on approximations in the [[Bauer–Fike theorem|Bauer-Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=Lloyd N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>l706uicyzocjzssh1i44u4k445zq1oj</sha1>
		</revision>
		<revision>
			<id>574624660</id>
			<parentid>574624282</parentid>
			<timestamp>2013-09-26T16:35:24Z</timestamp>
			<contributor>
				<ip>140.180.248.106</ip>
			</contributor>
			<comment>/* Steps */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9213">{{Multiple issues|
{{refimprove|date=April 2007}}
{{expert-subject|date=November 2008}}
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |authorlink=Lloyd N. Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_{i}) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left-multiplying equation (5) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0k}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinitesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==See also==

Bounds exist that do not rely on approximations in the [[Bauer–Fike theorem|Bauer-Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=Lloyd N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>j8myo7mxtl35y1osbhdqry9oes72j02</sha1>
		</revision>
		<revision>
			<id>574675609</id>
			<parentid>574624660</parentid>
			<timestamp>2013-09-27T00:05:50Z</timestamp>
			<contributor>
				<username>Arthur Rubin</username>
				<id>374195</id>
			</contributor>
			<comment>/* See also */ format section</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9141">{{Multiple issues|
{{refimprove|date=April 2007}}
{{expert-subject|date=November 2008}}
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |authorlink=Lloyd N. Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_{i}) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left-multiplying equation (5) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0k}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinitesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==See also==

* [[Bauer–Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=Lloyd N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]</text>
			<sha1>92g3xoe2qiwhmx3b59qw7hxyidrgy7u</sha1>
		</revision>
		<revision>
			<id>588397735</id>
			<parentid>574675609</parentid>
			<timestamp>2013-12-30T17:43:53Z</timestamp>
			<contributor>
				<username>طاها</username>
				<id>16608024</id>
			</contributor>
			<comment>added [[Category:Numerical linear algebra]] using
				[[WP:HC|HotCat]]</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9179">{{Multiple issues|
{{refimprove|date=April 2007}}
{{expert-subject|date=November 2008}}
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |authorlink=Lloyd N. Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_{i}) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are orthogonal, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left-multiplying equation (5) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0k}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinitesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==See also==

* [[Bauer–Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=Lloyd N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]
[[Category:Numerical linear algebra]]</text>
			<sha1>cfa5jyir27a6oqgwqzgg1clflvmipws</sha1>
		</revision>
		<revision>
			<id>594880297</id>
			<parentid>588397735</parentid>
			<timestamp>2014-02-10T20:52:11Z</timestamp>
			<contributor>
				<username>طاها</username>
				<id>16608024</id>
			</contributor>
			<comment>/* Steps */ More explanation for the step</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9239">{{Multiple issues|
{{refimprove|date=April 2007}}
{{expert-subject|date=November 2008}}
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |authorlink=Lloyd N. Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_{i}) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are &lt;math&gt;M_0&lt;/math&gt;-orthogonal when &lt;math&gt;M_0&lt;/math&gt; is positive definite, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left-multiplying equation (5) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0k}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinitesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==See also==

* [[Bauer–Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=Lloyd N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

[[Category:Perturbation theory]]
[[Category:Linear algebra]]
[[Category:Numerical linear algebra]]</text>
			<sha1>4uo05k9h77zu9uupbuld860gxwefsqv</sha1>
		</revision>
		<revision>
			<id>594890751</id>
			<parentid>594880297</parentid>
			<timestamp>2014-02-10T22:09:45Z</timestamp>
			<contributor>
				<username>طاها</username>
				<id>16608024</id>
			</contributor>
			<comment>/* References */ + Further Reading</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9452">{{Multiple issues|
{{refimprove|date=April 2007}}
{{expert-subject|date=November 2008}}
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |authorlink=Lloyd N. Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_{i}) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are &lt;math&gt;M_0&lt;/math&gt;-orthogonal when &lt;math&gt;M_0&lt;/math&gt; is positive definite, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left-multiplying equation (5) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0k}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinitesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==See also==

* [[Bauer–Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=Lloyd N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

== Further reading ==
* {{cite book|last=Hogben|first=[edited by] Leslie|title=Handbook of linear algebra|date=2014|isbn=1466507284|author=Ren-Cang Li|edition=Second edition.|chapter=Matrix Perturbation Theory}}

[[Category:Perturbation theory]]
[[Category:Linear algebra]]
[[Category:Numerical linear algebra]]</text>
			<sha1>ew3pim1vxkegpa66wosaw4ukw1ec5rf</sha1>
		</revision>
		<revision>
			<id>594891289</id>
			<parentid>594890751</parentid>
			<timestamp>2014-02-10T22:14:28Z</timestamp>
			<contributor>
				<username>طاها</username>
				<id>16608024</id>
			</contributor>
			<minor />
			<comment>/* Further reading */ Correction of citation</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9456">{{Multiple issues|
{{refimprove|date=April 2007}}
{{expert-subject|date=November 2008}}
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. |authorlink=Lloyd N. Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt;  or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know &lt;math&gt;\lambda_{0i}&lt;/math&gt; and &lt;math&gt;\mathbf{x}_{0i}&lt;/math&gt; for &lt;math&gt;i=1,\dots,N&lt;/math&gt;.  Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;[K] = [K_0]+[\delta K] \, &lt;/math&gt;

and

:&lt;math&gt;[M] = [M_0]+[\delta M] \, &lt;/math&gt;

where all of the &lt;math&gt;\delta&lt;/math&gt; terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\lambda_i = \lambda_{0i}+\delta\lambda_{0i} \, &lt;/math&gt;

and

:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}. \, &lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_{i}) = (\lambda_{0i}+\delta\lambda_{i})([M_0]+[\delta M])(\mathbf{x}_{0i}+\delta\mathbf{x}_{i}),&lt;/math&gt;

which expands to

:&lt;math&gt;
\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp; + [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\mathbf{x}_{0i}+
             \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;
\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i \\[6pt]
&amp; =   \lambda_{0i}[M_0]\delta\mathbf{x}_i + 
             \lambda_{0i}[\delta M]\mathbf{x}_{0i} +
             \delta\lambda_i[M_0]\mathbf{x}_{0i} \\[6pt]
&amp; {} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\mathbf{x}_{0i} +
           \delta\lambda_i[M_0]\delta\mathbf{x}_i + 
           \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}
&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad(4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:
:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are &lt;math&gt;M_0&lt;/math&gt;-orthogonal when &lt;math&gt;M_0&lt;/math&gt; is positive definite, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. ~~(6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i} ^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i}([\delta K] - \lambda_{0i}[\delta M] )\mathbf{x}_{0i}&lt;/math&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;■

Then, by left-multiplying equation (5) by &lt;math&gt;\mathbf{x}_{0k}&lt;/math&gt; (for &lt;math&gt;i\neq k&lt;/math&gt;):

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0k}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1 \Rightarrow \epsilon_{ii}=-\frac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinitesimal  &lt;math&gt;\delta K &lt;/math&gt; and &lt;math&gt;\delta M &lt;/math&gt; (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on &lt;math&gt;\lambda_i&lt;/math&gt; as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\frac{\partial \lambda_i}{\partial K_{(k\ell)}} = \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) = x_{0i(k)} x_{0i(\ell)} (2 - \delta_k^\ell) &lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \lambda_i}{\partial M_{(k\ell)}} = \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i}\right) =
\lambda_i x_{0i(k)} x_{0i(\ell)}(2-\delta_k^\ell).&lt;/math&gt;

Similarly

:&lt;math&gt;\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} = \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)}(2-\delta_k^\ell)}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

and

:&lt;math&gt;\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} = 
    -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - 
   \sum_{j=1\atop j\neq i}^N 
      \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}(2-\delta_k^\ell).&lt;/math&gt;

==See also==

* [[Bauer–Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=Lloyd N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

== Further reading ==
* {{cite book|editor1-last=Hogben|editor1-first=Leslie|title=Handbook of linear algebra|date=2014|isbn=1466507284|author=Ren-Cang Li|edition=Second edition.|chapter=Matrix Perturbation Theory}}

[[Category:Perturbation theory]]
[[Category:Linear algebra]]
[[Category:Numerical linear algebra]]</text>
			<sha1>2nmfpzfxfmh28gbej0g8o6xpv6e3izu</sha1>
		</revision>
		<revision>
			<id>608297114</id>
			<parentid>594891289</parentid>
			<timestamp>2014-05-12T23:43:14Z</timestamp>
			<contributor>
				<ip>99.241.166.168</ip>
			</contributor>
			<comment>formatting improvements and some latex to {{math|...}}</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="9503">{{Multiple issues|
{{refimprove|date=April 2007}}
{{expert-subject|date=November 2008}}
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. | authorlink=Lloyd N. Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt; or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;[K_0] \mathbf{x}_{0i} = \lambda_{0i} [M_0] \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know {{math|''λ''&lt;sub&gt;0''i''&lt;/sub&gt;}} and {{math|'''x'''&lt;sub&gt;0''i''&lt;/sub&gt;}} for {{math|''i'' {{=}} 1, ..., ''N''}}. Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;\begin{align}
{}[K] &amp;= [K_0]+[\delta K] \\
{} [M] &amp;= [M_0]+[\delta M]
\end{align}&lt;/math&gt;

where all of the {{mvar|δ}} terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\begin{align}
\lambda_i &amp;= \lambda_{0i}+\delta\lambda_{0i} \\
\mathbf{x}_i &amp;= \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}
\end{align}&lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top[M_0]\mathbf{x}_{0i} = \delta_i^j \qquad(2)&lt;/math&gt;

where &lt;math&gt;\delta_i^j&lt;/math&gt; is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;[K]\mathbf{x}_i = \lambda_i [M] \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;([K_0]+[\delta K])(\mathbf{x}_{0i} + \delta \mathbf{x}_{i}) = \left (\lambda_{0i}+\delta\lambda_{i} \right ) \left ([M_0]+[\delta M] \right ) \left (\mathbf{x}_{0i}+\delta\mathbf{x}_{i} \right ),&lt;/math&gt;

which expands to

:&lt;math&gt;\begin{align}
\left[K_0\right]\mathbf{x}_{0i} &amp;+ [\delta K]\mathbf{x}_{0i} + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i = \\[6pt]
&amp;=\lambda_{0i}[M_0]\mathbf{x}_{0i}+\lambda_{0i}[M_0]\delta\mathbf{x}_i + \lambda_{0i}[\delta M]\mathbf{x}_{0i} +\delta\lambda_i[M_0]\mathbf{x}_{0i} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + \delta\lambda_i [\delta M] \mathbf{x}_{0i} + \delta\lambda_i[M_0]\delta\mathbf{x}_i + \delta\lambda_i [\delta M] \delta\mathbf{x}_i.
\end{align}&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;\begin{align}
\left[\delta K\right]\mathbf{x}_{0i} &amp; + [K_0]\delta \mathbf{x}_i + [\delta K]\delta \mathbf{x}_i = \\[6pt]
&amp; = \lambda_{0i}[M_0]\delta\mathbf{x}_i + \lambda_{0i}[\delta M]\mathbf{x}_{0i} + \delta\lambda_i[M_0]\mathbf{x}_{0i} + \lambda_{0i}[\delta M]\delta\mathbf{x}_i + \delta\lambda_i[\delta M]\mathbf{x}_{0i} + \delta\lambda_i[M_0]\delta\mathbf{x}_i +  \delta\lambda_i[\delta M]\delta\mathbf{x}_i.
\end{align}&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;[K_0] \delta\mathbf{x}_i+[\delta K] \mathbf{x}_{0i} = \lambda_{0i}[M_0] \delta \mathbf{x}_i + \lambda_{0i}[\delta M]\mathrm{x}_{0i} + \delta \lambda_i [M_0]\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} \qquad (4)&lt;/math&gt;

where the &lt;math&gt;\epsilon_{ij}&lt;/math&gt; are small constants that are to be determined.  Substituting (4) into (3) and rearranging gives

:&lt;math&gt;[K_0]\sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. \qquad (5)&lt;/math&gt;

Or:

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} [K_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

By equation (1):

:&lt;math&gt;\sum_{j=1}^N \epsilon_{ij} \lambda_{0j} [M_0] \mathbf{x}_{0j} + [\delta K]\mathbf{x}_{0i} = \lambda_{0i} [M_0] \sum_{j=1}^N \epsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} [\delta M] \mathbf{x}_{0i} + \delta\lambda_i [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

Because the eigenvectors are {{math|''M''&lt;sub&gt;0&lt;/sub&gt;}}-orthogonal when {{math|''M''&lt;sub&gt;0&lt;/sub&gt;}} is positive definite, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \epsilon_{ii} \lambda_{0i} [M_0] \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0] \epsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[M_0] \epsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top [\delta M] \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}. \qquad (6) &lt;/math&gt;

The two terms containing &lt;math&gt;\epsilon_{ii}&lt;/math&gt; are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top[K_0]\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top[M_0]\mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top[\delta K]\mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top[\delta M] \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top [M_0] \mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i} \left ([\delta K] - \lambda_{0i}[\delta M] \right )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top[M_0] \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i} \left ([\delta K] - \lambda_{0i}[\delta M] \right )\mathbf{x}_{0i}.&lt;/math&gt;

Then, by left-multiplying equation (5) by {{math|'''x'''&lt;sub&gt;0''k''&lt;/sub&gt;}}:

:&lt;math&gt;\epsilon_{ik} = \frac{\mathbf{x}^\top_{0k} \left ([\delta K] - \lambda_{0i}[\delta M] \right )\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\epsilon_{ij} = \frac{\mathbf{x}^\top_{0j} \left ([\delta K] - \lambda_{0i}[\delta M] \right )\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find &lt;math&gt;\epsilon_{ii}&lt;/math&gt;, use the fact that:

:&lt;math&gt;\mathbf{x}^\top_i[M]\mathbf{x}_i = 1&lt;/math&gt;

implies:

:&lt;math&gt;\epsilon_{ii}=-\tfrac{1}{2}\mathbf{x}^\top_{0i}[\delta M]\mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\lambda_i = \lambda_{0i} + \mathbf{x}^\top_{0i} \left ([\delta K] - \lambda_{0i}[\delta M] \right ) \mathbf{x}_{0i}&lt;/math&gt;
and
:&lt;math&gt;\mathbf{x}_i = \mathbf{x}_{0i}(1 - \frac{1}{2} \mathbf{x}^\top_{0i}[\delta M] \mathbf{x}_{0i}) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}([\delta K] - \lambda_{0i}[\delta M])\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j}&lt;/math&gt;

for infinitesimal {{mvar|δK}} and {{mvar|δM}} (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on {{math|''λ&lt;sub&gt;i&lt;/sub&gt;''}} as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing &lt;math&gt;K_{(k\ell)}&lt;/math&gt; will also change &lt;math&gt;K_{(\ell k)}&lt;/math&gt;, hence the &lt;math&gt;(2-\delta_k^\ell)&lt;/math&gt; term.)

:&lt;math&gt;\begin{align}
\frac{\partial \lambda_i}{\partial K_{(k\ell)}} &amp;= \frac{\partial}{\partial K_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} ([\delta K] - \lambda_{0i}[\delta M]) \mathbf{x}_{0i} \right) = x_{0i(k)} x_{0i(\ell)} \left (2 - \delta_k^\ell \right ) \\
\frac{\partial \lambda_i}{\partial M_{(k\ell)}} &amp;= \frac{\partial}{\partial M_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} \left ([\delta K] - \lambda_{0i}[\delta M] \right ) \mathbf{x}_{0i}\right) = \lambda_i x_{0i(k)} x_{0i(\ell)} \left (2-\delta_k^\ell \right ).
\end{align}&lt;/math&gt;

Similarly

:&lt;math&gt;\begin{align}
\frac{\partial\mathbf{x}_i}{\partial K_{(k\ell)}} &amp;= \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)} \left (2-\delta_k^\ell \right )}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j} \\
\frac{\partial \mathbf{x}_i}{\partial M_{(k\ell)}} &amp;= -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_k^\ell) - \sum_{j=1\atop j\neq i}^N \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j} \left (2-\delta_k^\ell \right ).
\end{align}&lt;/math&gt;

==See also==
* [[Bauer–Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=Lloyd N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

== Further reading ==
* {{cite book|editor1-last=Hogben|editor1-first=Leslie|title=Handbook of linear algebra|date=2014|isbn=1466507284|author=Ren-Cang Li|edition=Second edition.|chapter=Matrix Perturbation Theory}}

[[Category:Perturbation theory]]
[[Category:Linear algebra]]
[[Category:Numerical linear algebra]]</text>
			<sha1>6ec2314i5fcirpg4ewqobykl7c1zfut</sha1>
		</revision>
		<revision>
			<id>608341850</id>
			<parentid>608297114</parentid>
			<timestamp>2014-05-13T06:45:07Z</timestamp>
			<contributor>
				<ip>99.241.166.168</ip>
			</contributor>
			<comment>replaced the bad notation for matrices with brackets with
				bold uppercase letters, changed \epsilon to \varepsilon and used
				align</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="10402">{{Multiple issues|
{{refimprove|date=April 2007}}
{{expert-subject|date=November 2008}}
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. | authorlink=Lloyd N. Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt; or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;\mathbf{K}_0 \mathbf{x}_{0i} = \lambda_{0i} \mathbf{M}_0 \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know {{math|''λ''&lt;sub&gt;0''i''&lt;/sub&gt;}} and {{math|'''x'''&lt;sub&gt;0''i''&lt;/sub&gt;}} for {{math|''i'' {{=}} 1, ..., ''N''}}. Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;\begin{align}
\mathbf{K} &amp;= \mathbf{K}_0 + \delta \mathbf{K} \\
\mathbf{M} &amp;= \mathbf{M}_0 + \delta \mathbf{M}
\end{align}&lt;/math&gt;

where all of the {{mvar|δ}} terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\begin{align}
\lambda_i &amp;= \lambda_{0i}+\delta\lambda_{0i} \\
\mathbf{x}_i &amp;= \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}
\end{align}&lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top \mathbf{M}_0\mathbf{x}_{0i} = \delta_{ij} \qquad(2)&lt;/math&gt;

where {{math|''δ&lt;sub&gt;ij&lt;/sub&gt;''}} is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;\mathbf{K}\mathbf{x}_i = \lambda_i \mathbf{M} \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;(\mathbf{K}_0+\delta \mathbf{K})(\mathbf{x}_{0i} + \delta \mathbf{x}_{i}) = \left (\lambda_{0i}+\delta\lambda_{i} \right ) \left (\mathbf{M}_0+ \delta \mathbf{M} \right ) \left (\mathbf{x}_{0i}+\delta\mathbf{x}_{i} \right ),&lt;/math&gt;

which expands to

:&lt;math&gt;\begin{align}
\mathbf{K}_0\mathbf{x}_{0i} &amp;+ \delta \mathbf{K}\mathbf{x}_{0i} + \mathbf{K}_0\delta \mathbf{x}_i + \delta \mathbf{K}\delta \mathbf{x}_i = \\[6pt]
&amp;=\lambda_{0i}\mathbf{M}_0\mathbf{x}_{0i}+\lambda_{0i}\mathbf{M}_0\delta\mathbf{x}_i + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} +\delta\lambda_i\mathbf{M}_0\mathbf{x}_{0i} + \lambda_{0i} \delta \mathbf{M} \delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M}\mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M} \delta\mathbf{x}_i.
\end{align}&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;\begin{align}
\mathbf{K} \mathbf{x}_{0i} + \mathbf{K}_0\delta \mathbf{x}_i + \delta \mathbf{K}\delta \mathbf{x}_i = \lambda_{0i}\mathbf{M}_0\delta\mathbf{x}_i + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\mathbf{x}_{0i} + \lambda_{0i} \delta \mathbf{M} \delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\delta\mathbf{x}_i +  \delta\lambda_i \delta \mathbf{M} \delta\mathbf{x}_i.
\end{align}&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;\mathbf{K}_0 \delta\mathbf{x}_i+ \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i}\mathbf{M}_0 \delta \mathbf{x}_i + \lambda_{0i}\delta \mathbf{M} \mathrm{x}_{0i} + \delta \lambda_i \mathbf{M}_0\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} \qquad (4)&lt;/math&gt;

where the {{math|''ε&lt;sub&gt;ij&lt;/sub&gt;''}} are small constants that are to be determined. Substituting (4) into (3) and rearranging gives

:&lt;math&gt;\begin{align}
\mathbf{K}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0\mathbf{x}_{0i} &amp;&amp; (5) \\
\sum_{j=1}^N \varepsilon_{ij} \mathbf{K}_0 \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0 \mathbf{x}_{0i} &amp;&amp; \text{Applying } \mathbf{K}_0 \text{ to the sum} \\
\sum_{j=1}^N \varepsilon_{ij} \lambda_{0j} \mathbf{M}_0 \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0 \mathbf{x}_{0i} &amp;&amp; \text{Using Eq. } (1) 
\end{align}&lt;/math&gt;

Because the eigenvectors are {{math|'''M'''&lt;sub&gt;0&lt;/sub&gt;}}-orthogonal when {{math|'''M'''&lt;sub&gt;0&lt;/sub&gt;}} is positive definite, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \varepsilon_{ii} \lambda_{0i} \mathbf{M}_0 \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \mathbf{M}_0 \varepsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top \mathbf{K}_0 \varepsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \mathbf{M}_0\varepsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top \delta \mathbf{M}\mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}. \qquad (6) &lt;/math&gt;

The two terms containing {{math|''ε&lt;sub&gt;ii&lt;/sub&gt;''}} are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top\mathbf{K}_0\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top \mathbf{M}_0\mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i} \left (\delta \mathbf{K}- \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top\mathbf{M}_0 \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}.&lt;/math&gt;

Then, by left-multiplying equation (5) by {{math|'''x'''&lt;sub&gt;0''k''&lt;/sub&gt;}}:

:&lt;math&gt;\varepsilon_{ik} = \frac{\mathbf{x}^\top_{0k} \left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right )\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\varepsilon_{ij} = \frac{\mathbf{x}^\top_{0j} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find {{math|''ε&lt;sub&gt;ii&lt;/sub&gt;''}}, use the fact that:

:&lt;math&gt;\mathbf{x}^\top_i \mathbf{M} \mathbf{x}_i = 1&lt;/math&gt;

implies:

:&lt;math&gt;\varepsilon_{ii}=-\tfrac{1}{2}\mathbf{x}^\top_{0i} \delta \mathbf{M} \mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\begin{align}
\lambda_i &amp;= \lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right ) \mathbf{x}_{0i} \\ 
\mathbf{x}_i &amp;= \mathbf{x}_{0i} \left (1 - \tfrac{1}{2} \mathbf{x}^\top_{0i} \delta \mathbf{M} \mathbf{x}_{0i} \right ) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}\left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right ) \mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}} \mathbf{x}_{0j}
\end{align}&lt;/math&gt;

for infinitesimal {{mvar|δK}} and {{mvar|δM}} (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on {{math|''λ&lt;sub&gt;i&lt;/sub&gt;''}} as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing {{math|'''K'''&lt;sub&gt;''k''ℓ&lt;/sub&gt;}} will also change {{math|'''K'''&lt;sub&gt;ℓ''k''&lt;/sub&gt;}}, hence the {{math|(2 − ''δ''&lt;sub&gt;''k''ℓ&lt;/sub&gt;)}} term.)

:&lt;math&gt;\begin{align}
\frac{\partial \lambda_i}{\partial \mathbf{K}_{(k\ell)}} &amp;= \frac{\partial}{\partial \mathbf{K}_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right ) \mathbf{x}_{0i} \right) = x_{0i(k)} x_{0i(\ell)} \left (2 - \delta_{k\ell} \right ) \\
\frac{\partial \lambda_i}{\partial \mathbf{M}_{(k\ell)}} &amp;= \frac{\partial}{\partial \mathbf{M}_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right ) \mathbf{x}_{0i}\right) = \lambda_i x_{0i(k)} x_{0i(\ell)} \left (2- \delta_{k\ell} \right ).
\end{align}&lt;/math&gt;

Similarly

:&lt;math&gt;\begin{align}
\frac{\partial\mathbf{x}_i}{\partial \mathbf{K}_{(k\ell)}} &amp;= \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)} \left (2-\delta_{k\ell} \right )}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j} \\
\frac{\partial \mathbf{x}_i}{\partial \mathbf{M}_{(k\ell)}} &amp;= -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_{k\ell}) - \sum_{j=1\atop j\neq i}^N \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j} \left (2-\delta_{k\ell} \right ).
\end{align}&lt;/math&gt;

==See also==
* [[Bauer–Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=Lloyd N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

== Further reading ==
* {{cite book|editor1-last=Hogben|editor1-first=Leslie|title=Handbook of linear algebra|date=2014|isbn=1466507284|author=Ren-Cang Li|edition=Second edition.|chapter=Matrix Perturbation Theory}}

[[Category:Perturbation theory]]
[[Category:Linear algebra]]
[[Category:Numerical linear algebra]]</text>
			<sha1>boh63hhx9t982i5o9pq8cscwektkjkl</sha1>
		</revision>
		<revision>
			<id>616386891</id>
			<parentid>608341850</parentid>
			<timestamp>2014-07-10T15:09:25Z</timestamp>
			<contributor>
				<username>Gundamlh</username>
				<id>16736905</id>
			</contributor>
			<comment>/* Steps */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="10409">{{Multiple issues|
{{refimprove|date=April 2007}}
{{expert-subject|date=November 2008}}
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. | authorlink=Lloyd N. Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt; or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;\mathbf{K}_0 \mathbf{x}_{0i} = \lambda_{0i} \mathbf{M}_0 \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know {{math|''λ''&lt;sub&gt;0''i''&lt;/sub&gt;}} and {{math|'''x'''&lt;sub&gt;0''i''&lt;/sub&gt;}} for {{math|''i'' {{=}} 1, ..., ''N''}}. Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;\begin{align}
\mathbf{K} &amp;= \mathbf{K}_0 + \delta \mathbf{K} \\
\mathbf{M} &amp;= \mathbf{M}_0 + \delta \mathbf{M}
\end{align}&lt;/math&gt;

where all of the {{mvar|δ}} terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\begin{align}
\lambda_i &amp;= \lambda_{0i}+\delta\lambda_{0i} \\
\mathbf{x}_i &amp;= \mathbf{x}_{0i} + \delta\mathbf{x}_{0i}
\end{align}&lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top \mathbf{M}_0\mathbf{x}_{0i} = \delta_{ij} \qquad(2)&lt;/math&gt;

where {{math|''δ&lt;sub&gt;ij&lt;/sub&gt;''}} is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;\mathbf{K}\mathbf{x}_i = \lambda_i \mathbf{M} \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;(\mathbf{K}_0+\delta \mathbf{K})(\mathbf{x}_{0i} + \delta \mathbf{x}_{i}) = \left (\lambda_{0i}+\delta\lambda_{i} \right ) \left (\mathbf{M}_0+ \delta \mathbf{M} \right ) \left (\mathbf{x}_{0i}+\delta\mathbf{x}_{i} \right ),&lt;/math&gt;

which expands to

:&lt;math&gt;\begin{align}
\mathbf{K}_0\mathbf{x}_{0i} &amp;+ \delta \mathbf{K}\mathbf{x}_{0i} + \mathbf{K}_0\delta \mathbf{x}_i + \delta \mathbf{K}\delta \mathbf{x}_i = \\[6pt]
&amp;=\lambda_{0i}\mathbf{M}_0\mathbf{x}_{0i}+\lambda_{0i}\mathbf{M}_0\delta\mathbf{x}_i + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} +\delta\lambda_i\mathbf{M}_0\mathbf{x}_{0i} + \lambda_{0i} \delta \mathbf{M} \delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M}\mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M} \delta\mathbf{x}_i.
\end{align}&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;\begin{align}
\delta \mathbf{K} \mathbf{x}_{0i} + \mathbf{K}_0\delta \mathbf{x}_i + \delta \mathbf{K}\delta \mathbf{x}_i = \lambda_{0i}\mathbf{M}_0\delta\mathbf{x}_i + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\mathbf{x}_{0i} + \lambda_{0i} \delta \mathbf{M} \delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\delta\mathbf{x}_i +  \delta\lambda_i \delta \mathbf{M} \delta\mathbf{x}_i.
\end{align}&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;\mathbf{K}_0 \delta\mathbf{x}_i+ \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i}\mathbf{M}_0 \delta \mathbf{x}_i + \lambda_{0i}\delta \mathbf{M} \mathrm{x}_{0i} + \delta \lambda_i \mathbf{M}_0\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} \qquad (4)&lt;/math&gt;

where the {{math|''ε&lt;sub&gt;ij&lt;/sub&gt;''}} are small constants that are to be determined. Substituting (4) into (3) and rearranging gives

:&lt;math&gt;\begin{align}
\mathbf{K}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0\mathbf{x}_{0i} &amp;&amp; (5) \\
\sum_{j=1}^N \varepsilon_{ij} \mathbf{K}_0 \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0 \mathbf{x}_{0i} &amp;&amp; \text{Applying } \mathbf{K}_0 \text{ to the sum} \\
\sum_{j=1}^N \varepsilon_{ij} \lambda_{0j} \mathbf{M}_0 \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0 \mathbf{x}_{0i} &amp;&amp; \text{Using Eq. } (1) 
\end{align}&lt;/math&gt;

Because the eigenvectors are {{math|'''M'''&lt;sub&gt;0&lt;/sub&gt;}}-orthogonal when {{math|'''M'''&lt;sub&gt;0&lt;/sub&gt;}} is positive definite, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \varepsilon_{ii} \lambda_{0i} \mathbf{M}_0 \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \mathbf{M}_0 \varepsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top \mathbf{K}_0 \varepsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \mathbf{M}_0\varepsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top \delta \mathbf{M}\mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}. \qquad (6) &lt;/math&gt;

The two terms containing {{math|''ε&lt;sub&gt;ii&lt;/sub&gt;''}} are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top\mathbf{K}_0\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top \mathbf{M}_0\mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i} \left (\delta \mathbf{K}- \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top\mathbf{M}_0 \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}.&lt;/math&gt;

Then, by left-multiplying equation (5) by {{math|'''x'''&lt;sub&gt;0''k''&lt;/sub&gt;}}:

:&lt;math&gt;\varepsilon_{ik} = \frac{\mathbf{x}^\top_{0k} \left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right )\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\varepsilon_{ij} = \frac{\mathbf{x}^\top_{0j} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find {{math|''ε&lt;sub&gt;ii&lt;/sub&gt;''}}, use the fact that:

:&lt;math&gt;\mathbf{x}^\top_i \mathbf{M} \mathbf{x}_i = 1&lt;/math&gt;

implies:

:&lt;math&gt;\varepsilon_{ii}=-\tfrac{1}{2}\mathbf{x}^\top_{0i} \delta \mathbf{M} \mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\begin{align}
\lambda_i &amp;= \lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right ) \mathbf{x}_{0i} \\ 
\mathbf{x}_i &amp;= \mathbf{x}_{0i} \left (1 - \tfrac{1}{2} \mathbf{x}^\top_{0i} \delta \mathbf{M} \mathbf{x}_{0i} \right ) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}\left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right ) \mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}} \mathbf{x}_{0j}
\end{align}&lt;/math&gt;

for infinitesimal {{mvar|δK}} and {{mvar|δM}} (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on {{math|''λ&lt;sub&gt;i&lt;/sub&gt;''}} as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing {{math|'''K'''&lt;sub&gt;''k''ℓ&lt;/sub&gt;}} will also change {{math|'''K'''&lt;sub&gt;ℓ''k''&lt;/sub&gt;}}, hence the {{math|(2 − ''δ''&lt;sub&gt;''k''ℓ&lt;/sub&gt;)}} term.)

:&lt;math&gt;\begin{align}
\frac{\partial \lambda_i}{\partial \mathbf{K}_{(k\ell)}} &amp;= \frac{\partial}{\partial \mathbf{K}_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right ) \mathbf{x}_{0i} \right) = x_{0i(k)} x_{0i(\ell)} \left (2 - \delta_{k\ell} \right ) \\
\frac{\partial \lambda_i}{\partial \mathbf{M}_{(k\ell)}} &amp;= \frac{\partial}{\partial \mathbf{M}_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right ) \mathbf{x}_{0i}\right) = \lambda_i x_{0i(k)} x_{0i(\ell)} \left (2- \delta_{k\ell} \right ).
\end{align}&lt;/math&gt;

Similarly

:&lt;math&gt;\begin{align}
\frac{\partial\mathbf{x}_i}{\partial \mathbf{K}_{(k\ell)}} &amp;= \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)} \left (2-\delta_{k\ell} \right )}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j} \\
\frac{\partial \mathbf{x}_i}{\partial \mathbf{M}_{(k\ell)}} &amp;= -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_{k\ell}) - \sum_{j=1\atop j\neq i}^N \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j} \left (2-\delta_{k\ell} \right ).
\end{align}&lt;/math&gt;

==See also==
* [[Bauer–Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=Lloyd N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

== Further reading ==
* {{cite book|editor1-last=Hogben|editor1-first=Leslie|title=Handbook of linear algebra|date=2014|isbn=1466507284|author=Ren-Cang Li|edition=Second edition.|chapter=Matrix Perturbation Theory}}

[[Category:Perturbation theory]]
[[Category:Linear algebra]]
[[Category:Numerical linear algebra]]</text>
			<sha1>1teswm6ovy8pt1a3snbclf0s2vapp27</sha1>
		</revision>
		<revision>
			<id>616387012</id>
			<parentid>616386891</parentid>
			<timestamp>2014-07-10T15:10:14Z</timestamp>
			<contributor>
				<username>Gundamlh</username>
				<id>16736905</id>
			</contributor>
			<comment>/* Example */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="10407">{{Multiple issues|
{{refimprove|date=April 2007}}
{{expert-subject|date=November 2008}}
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. | authorlink=Lloyd N. Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt; or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;\mathbf{K}_0 \mathbf{x}_{0i} = \lambda_{0i} \mathbf{M}_0 \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know {{math|''λ''&lt;sub&gt;0''i''&lt;/sub&gt;}} and {{math|'''x'''&lt;sub&gt;0''i''&lt;/sub&gt;}} for {{math|''i'' {{=}} 1, ..., ''N''}}. Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;\begin{align}
\mathbf{K} &amp;= \mathbf{K}_0 + \delta \mathbf{K} \\
\mathbf{M} &amp;= \mathbf{M}_0 + \delta \mathbf{M}
\end{align}&lt;/math&gt;

where all of the {{mvar|δ}} terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\begin{align}
\lambda_i &amp;= \lambda_{0i}+\delta\lambda_{i} \\
\mathbf{x}_i &amp;= \mathbf{x}_{0i} + \delta\mathbf{x}_{i}
\end{align}&lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top \mathbf{M}_0\mathbf{x}_{0i} = \delta_{ij} \qquad(2)&lt;/math&gt;

where {{math|''δ&lt;sub&gt;ij&lt;/sub&gt;''}} is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;\mathbf{K}\mathbf{x}_i = \lambda_i \mathbf{M} \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;(\mathbf{K}_0+\delta \mathbf{K})(\mathbf{x}_{0i} + \delta \mathbf{x}_{i}) = \left (\lambda_{0i}+\delta\lambda_{i} \right ) \left (\mathbf{M}_0+ \delta \mathbf{M} \right ) \left (\mathbf{x}_{0i}+\delta\mathbf{x}_{i} \right ),&lt;/math&gt;

which expands to

:&lt;math&gt;\begin{align}
\mathbf{K}_0\mathbf{x}_{0i} &amp;+ \delta \mathbf{K}\mathbf{x}_{0i} + \mathbf{K}_0\delta \mathbf{x}_i + \delta \mathbf{K}\delta \mathbf{x}_i = \\[6pt]
&amp;=\lambda_{0i}\mathbf{M}_0\mathbf{x}_{0i}+\lambda_{0i}\mathbf{M}_0\delta\mathbf{x}_i + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} +\delta\lambda_i\mathbf{M}_0\mathbf{x}_{0i} + \lambda_{0i} \delta \mathbf{M} \delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M}\mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M} \delta\mathbf{x}_i.
\end{align}&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;\begin{align}
\delta \mathbf{K} \mathbf{x}_{0i} + \mathbf{K}_0\delta \mathbf{x}_i + \delta \mathbf{K}\delta \mathbf{x}_i = \lambda_{0i}\mathbf{M}_0\delta\mathbf{x}_i + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\mathbf{x}_{0i} + \lambda_{0i} \delta \mathbf{M} \delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\delta\mathbf{x}_i +  \delta\lambda_i \delta \mathbf{M} \delta\mathbf{x}_i.
\end{align}&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;\mathbf{K}_0 \delta\mathbf{x}_i+ \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i}\mathbf{M}_0 \delta \mathbf{x}_i + \lambda_{0i}\delta \mathbf{M} \mathrm{x}_{0i} + \delta \lambda_i \mathbf{M}_0\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} \qquad (4)&lt;/math&gt;

where the {{math|''ε&lt;sub&gt;ij&lt;/sub&gt;''}} are small constants that are to be determined. Substituting (4) into (3) and rearranging gives

:&lt;math&gt;\begin{align}
\mathbf{K}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0\mathbf{x}_{0i} &amp;&amp; (5) \\
\sum_{j=1}^N \varepsilon_{ij} \mathbf{K}_0 \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0 \mathbf{x}_{0i} &amp;&amp; \text{Applying } \mathbf{K}_0 \text{ to the sum} \\
\sum_{j=1}^N \varepsilon_{ij} \lambda_{0j} \mathbf{M}_0 \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0 \mathbf{x}_{0i} &amp;&amp; \text{Using Eq. } (1) 
\end{align}&lt;/math&gt;

Because the eigenvectors are {{math|'''M'''&lt;sub&gt;0&lt;/sub&gt;}}-orthogonal when {{math|'''M'''&lt;sub&gt;0&lt;/sub&gt;}} is positive definite, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \varepsilon_{ii} \lambda_{0i} \mathbf{M}_0 \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \mathbf{M}_0 \varepsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top \mathbf{K}_0 \varepsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \mathbf{M}_0\varepsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top \delta \mathbf{M}\mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}. \qquad (6) &lt;/math&gt;

The two terms containing {{math|''ε&lt;sub&gt;ii&lt;/sub&gt;''}} are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top\mathbf{K}_0\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top \mathbf{M}_0\mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i} \left (\delta \mathbf{K}- \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top\mathbf{M}_0 \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}.&lt;/math&gt;

Then, by left-multiplying equation (5) by {{math|'''x'''&lt;sub&gt;0''k''&lt;/sub&gt;}}:

:&lt;math&gt;\varepsilon_{ik} = \frac{\mathbf{x}^\top_{0k} \left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right )\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\varepsilon_{ij} = \frac{\mathbf{x}^\top_{0j} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find {{math|''ε&lt;sub&gt;ii&lt;/sub&gt;''}}, use the fact that:

:&lt;math&gt;\mathbf{x}^\top_i \mathbf{M} \mathbf{x}_i = 1&lt;/math&gt;

implies:

:&lt;math&gt;\varepsilon_{ii}=-\tfrac{1}{2}\mathbf{x}^\top_{0i} \delta \mathbf{M} \mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\begin{align}
\lambda_i &amp;= \lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right ) \mathbf{x}_{0i} \\ 
\mathbf{x}_i &amp;= \mathbf{x}_{0i} \left (1 - \tfrac{1}{2} \mathbf{x}^\top_{0i} \delta \mathbf{M} \mathbf{x}_{0i} \right ) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}\left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right ) \mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}} \mathbf{x}_{0j}
\end{align}&lt;/math&gt;

for infinitesimal {{mvar|δK}} and {{mvar|δM}} (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on {{math|''λ&lt;sub&gt;i&lt;/sub&gt;''}} as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing {{math|'''K'''&lt;sub&gt;''k''ℓ&lt;/sub&gt;}} will also change {{math|'''K'''&lt;sub&gt;ℓ''k''&lt;/sub&gt;}}, hence the {{math|(2 − ''δ''&lt;sub&gt;''k''ℓ&lt;/sub&gt;)}} term.)

:&lt;math&gt;\begin{align}
\frac{\partial \lambda_i}{\partial \mathbf{K}_{(k\ell)}} &amp;= \frac{\partial}{\partial \mathbf{K}_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right ) \mathbf{x}_{0i} \right) = x_{0i(k)} x_{0i(\ell)} \left (2 - \delta_{k\ell} \right ) \\
\frac{\partial \lambda_i}{\partial \mathbf{M}_{(k\ell)}} &amp;= \frac{\partial}{\partial \mathbf{M}_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right ) \mathbf{x}_{0i}\right) = \lambda_i x_{0i(k)} x_{0i(\ell)} \left (2- \delta_{k\ell} \right ).
\end{align}&lt;/math&gt;

Similarly

:&lt;math&gt;\begin{align}
\frac{\partial\mathbf{x}_i}{\partial \mathbf{K}_{(k\ell)}} &amp;= \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)} \left (2-\delta_{k\ell} \right )}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j} \\
\frac{\partial \mathbf{x}_i}{\partial \mathbf{M}_{(k\ell)}} &amp;= -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_{k\ell}) - \sum_{j=1\atop j\neq i}^N \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j} \left (2-\delta_{k\ell} \right ).
\end{align}&lt;/math&gt;

==See also==
* [[Bauer–Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=Lloyd N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

== Further reading ==
* {{cite book|editor1-last=Hogben|editor1-first=Leslie|title=Handbook of linear algebra|date=2014|isbn=1466507284|author=Ren-Cang Li|edition=Second edition.|chapter=Matrix Perturbation Theory}}

[[Category:Perturbation theory]]
[[Category:Linear algebra]]
[[Category:Numerical linear algebra]]</text>
			<sha1>0jx3zhvzgpewvv781yv3aul1vgjahav</sha1>
		</revision>
		<revision>
			<id>627887517</id>
			<parentid>616387012</parentid>
			<timestamp>2014-10-01T23:46:52Z</timestamp>
			<contributor>
				<username>Cuzkatzimhut</username>
				<id>2996924</id>
			</contributor>
			<comment>/* See also */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="10453">{{Multiple issues|
{{refimprove|date=April 2007}}
{{expert-subject|date=November 2008}}
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. | authorlink=Lloyd N. Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt; or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;\mathbf{K}_0 \mathbf{x}_{0i} = \lambda_{0i} \mathbf{M}_0 \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know {{math|''λ''&lt;sub&gt;0''i''&lt;/sub&gt;}} and {{math|'''x'''&lt;sub&gt;0''i''&lt;/sub&gt;}} for {{math|''i'' {{=}} 1, ..., ''N''}}. Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;\begin{align}
\mathbf{K} &amp;= \mathbf{K}_0 + \delta \mathbf{K} \\
\mathbf{M} &amp;= \mathbf{M}_0 + \delta \mathbf{M}
\end{align}&lt;/math&gt;

where all of the {{mvar|δ}} terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\begin{align}
\lambda_i &amp;= \lambda_{0i}+\delta\lambda_{i} \\
\mathbf{x}_i &amp;= \mathbf{x}_{0i} + \delta\mathbf{x}_{i}
\end{align}&lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top \mathbf{M}_0\mathbf{x}_{0i} = \delta_{ij} \qquad(2)&lt;/math&gt;

where {{math|''δ&lt;sub&gt;ij&lt;/sub&gt;''}} is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;\mathbf{K}\mathbf{x}_i = \lambda_i \mathbf{M} \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;(\mathbf{K}_0+\delta \mathbf{K})(\mathbf{x}_{0i} + \delta \mathbf{x}_{i}) = \left (\lambda_{0i}+\delta\lambda_{i} \right ) \left (\mathbf{M}_0+ \delta \mathbf{M} \right ) \left (\mathbf{x}_{0i}+\delta\mathbf{x}_{i} \right ),&lt;/math&gt;

which expands to

:&lt;math&gt;\begin{align}
\mathbf{K}_0\mathbf{x}_{0i} &amp;+ \delta \mathbf{K}\mathbf{x}_{0i} + \mathbf{K}_0\delta \mathbf{x}_i + \delta \mathbf{K}\delta \mathbf{x}_i = \\[6pt]
&amp;=\lambda_{0i}\mathbf{M}_0\mathbf{x}_{0i}+\lambda_{0i}\mathbf{M}_0\delta\mathbf{x}_i + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} +\delta\lambda_i\mathbf{M}_0\mathbf{x}_{0i} + \lambda_{0i} \delta \mathbf{M} \delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M}\mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M} \delta\mathbf{x}_i.
\end{align}&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;\begin{align}
\delta \mathbf{K} \mathbf{x}_{0i} + \mathbf{K}_0\delta \mathbf{x}_i + \delta \mathbf{K}\delta \mathbf{x}_i = \lambda_{0i}\mathbf{M}_0\delta\mathbf{x}_i + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\mathbf{x}_{0i} + \lambda_{0i} \delta \mathbf{M} \delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\delta\mathbf{x}_i +  \delta\lambda_i \delta \mathbf{M} \delta\mathbf{x}_i.
\end{align}&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;\mathbf{K}_0 \delta\mathbf{x}_i+ \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i}\mathbf{M}_0 \delta \mathbf{x}_i + \lambda_{0i}\delta \mathbf{M} \mathrm{x}_{0i} + \delta \lambda_i \mathbf{M}_0\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} \qquad (4)&lt;/math&gt;

where the {{math|''ε&lt;sub&gt;ij&lt;/sub&gt;''}} are small constants that are to be determined. Substituting (4) into (3) and rearranging gives

:&lt;math&gt;\begin{align}
\mathbf{K}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0\mathbf{x}_{0i} &amp;&amp; (5) \\
\sum_{j=1}^N \varepsilon_{ij} \mathbf{K}_0 \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0 \mathbf{x}_{0i} &amp;&amp; \text{Applying } \mathbf{K}_0 \text{ to the sum} \\
\sum_{j=1}^N \varepsilon_{ij} \lambda_{0j} \mathbf{M}_0 \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0 \mathbf{x}_{0i} &amp;&amp; \text{Using Eq. } (1) 
\end{align}&lt;/math&gt;

Because the eigenvectors are {{math|'''M'''&lt;sub&gt;0&lt;/sub&gt;}}-orthogonal when {{math|'''M'''&lt;sub&gt;0&lt;/sub&gt;}} is positive definite, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \varepsilon_{ii} \lambda_{0i} \mathbf{M}_0 \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \mathbf{M}_0 \varepsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top \mathbf{K}_0 \varepsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \mathbf{M}_0\varepsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top \delta \mathbf{M}\mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}. \qquad (6) &lt;/math&gt;

The two terms containing {{math|''ε&lt;sub&gt;ii&lt;/sub&gt;''}} are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top\mathbf{K}_0\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top \mathbf{M}_0\mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i} \left (\delta \mathbf{K}- \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top\mathbf{M}_0 \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}.&lt;/math&gt;

Then, by left-multiplying equation (5) by {{math|'''x'''&lt;sub&gt;0''k''&lt;/sub&gt;}}:

:&lt;math&gt;\varepsilon_{ik} = \frac{\mathbf{x}^\top_{0k} \left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right )\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\varepsilon_{ij} = \frac{\mathbf{x}^\top_{0j} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find {{math|''ε&lt;sub&gt;ii&lt;/sub&gt;''}}, use the fact that:

:&lt;math&gt;\mathbf{x}^\top_i \mathbf{M} \mathbf{x}_i = 1&lt;/math&gt;

implies:

:&lt;math&gt;\varepsilon_{ii}=-\tfrac{1}{2}\mathbf{x}^\top_{0i} \delta \mathbf{M} \mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\begin{align}
\lambda_i &amp;= \lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right ) \mathbf{x}_{0i} \\ 
\mathbf{x}_i &amp;= \mathbf{x}_{0i} \left (1 - \tfrac{1}{2} \mathbf{x}^\top_{0i} \delta \mathbf{M} \mathbf{x}_{0i} \right ) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}\left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right ) \mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}} \mathbf{x}_{0j}
\end{align}&lt;/math&gt;

for infinitesimal {{mvar|δK}} and {{mvar|δM}} (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on {{math|''λ&lt;sub&gt;i&lt;/sub&gt;''}} as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing {{math|'''K'''&lt;sub&gt;''k''ℓ&lt;/sub&gt;}} will also change {{math|'''K'''&lt;sub&gt;ℓ''k''&lt;/sub&gt;}}, hence the {{math|(2 − ''δ''&lt;sub&gt;''k''ℓ&lt;/sub&gt;)}} term.)

:&lt;math&gt;\begin{align}
\frac{\partial \lambda_i}{\partial \mathbf{K}_{(k\ell)}} &amp;= \frac{\partial}{\partial \mathbf{K}_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right ) \mathbf{x}_{0i} \right) = x_{0i(k)} x_{0i(\ell)} \left (2 - \delta_{k\ell} \right ) \\
\frac{\partial \lambda_i}{\partial \mathbf{M}_{(k\ell)}} &amp;= \frac{\partial}{\partial \mathbf{M}_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right ) \mathbf{x}_{0i}\right) = \lambda_i x_{0i(k)} x_{0i(\ell)} \left (2- \delta_{k\ell} \right ).
\end{align}&lt;/math&gt;

Similarly

:&lt;math&gt;\begin{align}
\frac{\partial\mathbf{x}_i}{\partial \mathbf{K}_{(k\ell)}} &amp;= \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)} \left (2-\delta_{k\ell} \right )}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j} \\
\frac{\partial \mathbf{x}_i}{\partial \mathbf{M}_{(k\ell)}} &amp;= -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_{k\ell}) - \sum_{j=1\atop j\neq i}^N \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j} \left (2-\delta_{k\ell} \right ).
\end{align}&lt;/math&gt;

==See also==
* [[Perturbation theory (quantum mechanics)]]
* [[Bauer–Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=Lloyd N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

== Further reading ==
* {{cite book|editor1-last=Hogben|editor1-first=Leslie|title=Handbook of linear algebra|date=2014|isbn=1466507284|author=Ren-Cang Li|edition=Second edition.|chapter=Matrix Perturbation Theory}}

[[Category:Perturbation theory]]
[[Category:Linear algebra]]
[[Category:Numerical linear algebra]]</text>
			<sha1>5m1uam8srhghgdpgk7kn4eq8clarfeh</sha1>
		</revision>
		<revision>
			<id>627951210</id>
			<parentid>627887517</parentid>
			<timestamp>2014-10-02T13:30:51Z</timestamp>
			<contributor>
				<username>Cuzkatzimhut</username>
				<id>2996924</id>
			</contributor>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="10851">{{main|Perturbation theory}}
{{Multiple issues|
{{refimprove|date=April 2007}}
{{expert-subject|date=November 2008}}
}}

In mathematics, '''eigenvalue perturbation''' is a [[perturbation theory|perturbation]] approach to finding [[Eigenvector|eigenvalues and eigenvectors]] of systems perturbed from one with known eigenvectors and eigenvalues. It also allows one to determine the sensitivity of the eigenvalues and eigenvectors with respect to changes in the system.
In this form, it was popularized by [[John William Strutt, 3rd Baron Rayleigh|Lord Rayleigh]],&lt;ref&gt;{{cite book|first=J. W. S.|last=Rayleigh|title=Theory of Sound|edition=2nd |volume= I|pages=115–118|publisher= Macmillan|location= London |year=1894|isbn=1-152-06023-6}}&lt;/ref&gt; in his investigation of harmonic vibrations of a string perturbed by small inhomogeneities. The following derivations are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. | authorlink=Lloyd N. Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt; or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;\mathbf{K}_0 \mathbf{x}_{0i} = \lambda_{0i} \mathbf{M}_0 \mathbf{x}_{0i}. \qquad (1)&lt;/math&gt;

That is, we know {{math|''λ''&lt;sub&gt;0''i''&lt;/sub&gt;}} and {{math|'''x'''&lt;sub&gt;0''i''&lt;/sub&gt;}} for {{math|''i'' {{=}} 1, ..., ''N''}}. Now suppose we want to change the matrices by a small amount. That is, we want to let

:&lt;math&gt;\begin{align}
\mathbf{K} &amp;= \mathbf{K}_0 + \delta \mathbf{K} \\
\mathbf{M} &amp;= \mathbf{M}_0 + \delta \mathbf{M}
\end{align}&lt;/math&gt;

where all of the {{mvar|δ}} terms are much smaller than the corresponding term. We expect answers to be of the form

:&lt;math&gt;\begin{align}
\lambda_i &amp;= \lambda_{0i}+\delta\lambda_{i} \\
\mathbf{x}_i &amp;= \mathbf{x}_{0i} + \delta\mathbf{x}_{i}
\end{align}&lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]] and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top \mathbf{M}_0\mathbf{x}_{0i} = \delta_{ij} \qquad(2)&lt;/math&gt;

where {{math|''δ&lt;sub&gt;ij&lt;/sub&gt;''}} is the [[Kronecker delta]].

Now we want to solve the equation

:&lt;math&gt;\mathbf{K}\mathbf{x}_i = \lambda_i \mathbf{M} \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;(\mathbf{K}_0+\delta \mathbf{K})(\mathbf{x}_{0i} + \delta \mathbf{x}_{i}) = \left (\lambda_{0i}+\delta\lambda_{i} \right ) \left (\mathbf{M}_0+ \delta \mathbf{M} \right ) \left (\mathbf{x}_{0i}+\delta\mathbf{x}_{i} \right ),&lt;/math&gt;

which expands to

:&lt;math&gt;\begin{align}
\mathbf{K}_0\mathbf{x}_{0i} &amp;+ \delta \mathbf{K}\mathbf{x}_{0i} + \mathbf{K}_0\delta \mathbf{x}_i + \delta \mathbf{K}\delta \mathbf{x}_i = \\[6pt]
&amp;=\lambda_{0i}\mathbf{M}_0\mathbf{x}_{0i}+\lambda_{0i}\mathbf{M}_0\delta\mathbf{x}_i + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} +\delta\lambda_i\mathbf{M}_0\mathbf{x}_{0i} + \lambda_{0i} \delta \mathbf{M} \delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M}\mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M} \delta\mathbf{x}_i.
\end{align}&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;\begin{align}
\delta \mathbf{K} \mathbf{x}_{0i} + \mathbf{K}_0\delta \mathbf{x}_i + \delta \mathbf{K}\delta \mathbf{x}_i = \lambda_{0i}\mathbf{M}_0\delta\mathbf{x}_i + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\mathbf{x}_{0i} + \lambda_{0i} \delta \mathbf{M} \delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\delta\mathbf{x}_i +  \delta\lambda_i \delta \mathbf{M} \delta\mathbf{x}_i.
\end{align}&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;\mathbf{K}_0 \delta\mathbf{x}_i+ \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i}\mathbf{M}_0 \delta \mathbf{x}_i + \lambda_{0i}\delta \mathbf{M} \mathrm{x}_{0i} + \delta \lambda_i \mathbf{M}_0\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} \qquad (4)&lt;/math&gt;

where the {{math|''ε&lt;sub&gt;ij&lt;/sub&gt;''}} are small constants that are to be determined. Substituting (4) into (3) and rearranging gives

:&lt;math&gt;\begin{align}
\mathbf{K}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0\mathbf{x}_{0i} &amp;&amp; (5) \\
\sum_{j=1}^N \varepsilon_{ij} \mathbf{K}_0 \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0 \mathbf{x}_{0i} &amp;&amp; \text{Applying } \mathbf{K}_0 \text{ to the sum} \\
\sum_{j=1}^N \varepsilon_{ij} \lambda_{0j} \mathbf{M}_0 \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0 \mathbf{x}_{0i} &amp;&amp; \text{Using Eq. } (1) 
\end{align}&lt;/math&gt;

Because the eigenvectors are {{math|'''M'''&lt;sub&gt;0&lt;/sub&gt;}}-orthogonal when {{math|'''M'''&lt;sub&gt;0&lt;/sub&gt;}} is positive definite, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \varepsilon_{ii} \lambda_{0i} \mathbf{M}_0 \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \mathbf{M}_0 \varepsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top \mathbf{K}_0 \varepsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \mathbf{M}_0\varepsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top \delta \mathbf{M}\mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}. \qquad (6) &lt;/math&gt;

The two terms containing {{math|''ε&lt;sub&gt;ii&lt;/sub&gt;''}} are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top\mathbf{K}_0\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top \mathbf{M}_0\mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i} \left (\delta \mathbf{K}- \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top\mathbf{M}_0 \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}.&lt;/math&gt;

Then, by left-multiplying equation (5) by {{math|'''x'''&lt;sub&gt;0''k''&lt;/sub&gt;}}:

:&lt;math&gt;\varepsilon_{ik} = \frac{\mathbf{x}^\top_{0k} \left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right )\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\varepsilon_{ij} = \frac{\mathbf{x}^\top_{0j} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find {{math|''ε&lt;sub&gt;ii&lt;/sub&gt;''}}, use the fact that:

:&lt;math&gt;\mathbf{x}^\top_i \mathbf{M} \mathbf{x}_i = 1&lt;/math&gt;

implies:

:&lt;math&gt;\varepsilon_{ii}=-\tfrac{1}{2}\mathbf{x}^\top_{0i} \delta \mathbf{M} \mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\begin{align}
\lambda_i &amp;= \lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right ) \mathbf{x}_{0i} \\ 
\mathbf{x}_i &amp;= \mathbf{x}_{0i} \left (1 - \tfrac{1}{2} \mathbf{x}^\top_{0i} \delta \mathbf{M} \mathbf{x}_{0i} \right ) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}\left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right ) \mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}} \mathbf{x}_{0j}
\end{align}&lt;/math&gt;

for infinitesimal {{mvar|δK}} and {{mvar|δM}} (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on {{math|''λ&lt;sub&gt;i&lt;/sub&gt;''}} as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing {{math|'''K'''&lt;sub&gt;''k''ℓ&lt;/sub&gt;}} will also change {{math|'''K'''&lt;sub&gt;ℓ''k''&lt;/sub&gt;}}, hence the {{math|(2 − ''δ''&lt;sub&gt;''k''ℓ&lt;/sub&gt;)}} term.)

:&lt;math&gt;\begin{align}
\frac{\partial \lambda_i}{\partial \mathbf{K}_{(k\ell)}} &amp;= \frac{\partial}{\partial \mathbf{K}_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right ) \mathbf{x}_{0i} \right) = x_{0i(k)} x_{0i(\ell)} \left (2 - \delta_{k\ell} \right ) \\
\frac{\partial \lambda_i}{\partial \mathbf{M}_{(k\ell)}} &amp;= \frac{\partial}{\partial \mathbf{M}_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right ) \mathbf{x}_{0i}\right) = \lambda_i x_{0i(k)} x_{0i(\ell)} \left (2- \delta_{k\ell} \right ).
\end{align}&lt;/math&gt;

Similarly

:&lt;math&gt;\begin{align}
\frac{\partial\mathbf{x}_i}{\partial \mathbf{K}_{(k\ell)}} &amp;= \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)} \left (2-\delta_{k\ell} \right )}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j} \\
\frac{\partial \mathbf{x}_i}{\partial \mathbf{M}_{(k\ell)}} &amp;= -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_{k\ell}) - \sum_{j=1\atop j\neq i}^N \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j} \left (2-\delta_{k\ell} \right ).
\end{align}&lt;/math&gt;

==See also==
* [[Perturbation theory (quantum mechanics)]]
* [[Bauer–Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=Lloyd N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

== Further reading ==
* {{cite book|editor1-last=Hogben|editor1-first=Leslie|title=Handbook of linear algebra|date=2014|isbn=1466507284|author=Ren-Cang Li|edition=Second edition.|chapter=Matrix Perturbation Theory}}

[[Category:Perturbation theory]]
[[Category:Linear algebra]]
[[Category:Numerical linear algebra]]</text>
			<sha1>qi4j1hem2s9ajkkixvgavg7ui56n8v7</sha1>
		</revision>
		<revision>
			<id>642217981</id>
			<parentid>627951210</parentid>
			<timestamp>2015-01-12T22:11:56Z</timestamp>
			<contributor>
				<username>Jftsang</username>
				<id>164130</id>
			</contributor>
			<comment>Rewrite lead section, clarify example setup, add notes</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="11761">{{main|Perturbation theory}}
{{Multiple issues|
{{refimprove|date=April 2007}}
{{expert-subject|date=November 2008}}
}}

In mathematics, an '''eigenvalue perturbation''' problem is that of finding the [[eigenvector|eigenvectors and eigenvalues]] of a system that is [[perturbation theory|perturbed]] from one with known eigenvectors and eigenvalues. This is useful for studying how sensitive the original system's eigenvectors and eigenvalues are to changes in the system. 
This type of analysis popularized by [[John William Strutt, 3rd Baron Rayleigh|Lord Rayleigh]], in his investigation of harmonic vibrations of a string perturbed by small inhomogeneities.&lt;ref&gt;{{cite book|first=J. W. S.|last=Rayleigh|title=Theory of Sound|edition=2nd |volume= I|pages=115–118|publisher= Macmillan|location= London |year=1894|isbn=1-152-06023-6}}&lt;/ref&gt; 

The derivations in this article are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. | authorlink=Lloyd N. Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt; or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;\mathbf{K}_0 \mathbf{x}_{0i} = \lambda_{0i} \mathbf{M}_0 \mathbf{x}_{0i}. \qquad (0)&lt;/math&gt;

where &lt;math&gt;\mathbf{K}&lt;/math&gt; and &lt;math&gt;\mathbf{M}&lt;/math&gt; are matrices. That is, we know the eigenvalues {{math|''λ''&lt;sub&gt;0''i''&lt;/sub&gt;}} and eigenvectors {{math|'''x'''&lt;sub&gt;0''i''&lt;/sub&gt;}} for {{math|''i'' {{=}} 1, ..., ''N''}}. Now suppose we want to change the matrices by a small amount. That is, we want to find the eigenvalues and eigenvectors of
:&lt;math&gt;\mathbf{K} \mathbf{x}_i = \lambda_i \mathbf{M} \mathbf{x}_i \qquad (1)&lt;/math&gt;
where
:&lt;math&gt;\begin{align}
\mathbf{K} &amp;= \mathbf{K}_0 + \delta \mathbf{K}\\
\mathbf{M} &amp;= \mathbf{M}_0 + \delta \mathbf{M}
\end{align}&lt;/math&gt;

with the perturbations &lt;math&gt;\delta\mathbf{K}&lt;/math&gt; and &lt;math&gt;\delta\mathbf{M}&lt;/math&gt; much smaller than &lt;math&gt;\mathbf{K}&lt;/math&gt; and &lt;math&gt;\mathbf{M}&lt;/math&gt; respectively. Then we expect the new eigenvalues and eigenvectors to be similar to the original, plus small perturbations:

:&lt;math&gt;\begin{align}
\lambda_i &amp;= \lambda_{0i}+\delta\lambda_{i} \\
\mathbf{x}_i &amp;= \mathbf{x}_{0i} + \delta\mathbf{x}_{i} 
\end{align}&lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]], and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top \mathbf{M}_0\mathbf{x}_{0i} = \delta_{ij} \qquad(2)&lt;/math&gt;

where {{math|''δ&lt;sub&gt;ij&lt;/sub&gt;''}} is the [[Kronecker delta]]. 
Now we want to solve the equation

:&lt;math&gt;\mathbf{K}\mathbf{x}_i = \lambda_i \mathbf{M} \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;(\mathbf{K}_0+\delta \mathbf{K})(\mathbf{x}_{0i} + \delta \mathbf{x}_{i}) = \left (\lambda_{0i}+\delta\lambda_{i} \right ) \left (\mathbf{M}_0+ \delta \mathbf{M} \right ) \left (\mathbf{x}_{0i}+\delta\mathbf{x}_{i} \right ),&lt;/math&gt;

which expands to

:&lt;math&gt;\begin{align}
\mathbf{K}_0\mathbf{x}_{0i} &amp;+ \delta \mathbf{K}\mathbf{x}_{0i} + \mathbf{K}_0\delta \mathbf{x}_i + \delta \mathbf{K}\delta \mathbf{x}_i = \\[6pt]
&amp;=\lambda_{0i}\mathbf{M}_0\mathbf{x}_{0i}+\lambda_{0i}\mathbf{M}_0\delta\mathbf{x}_i + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} +\delta\lambda_i\mathbf{M}_0\mathbf{x}_{0i} + \lambda_{0i} \delta \mathbf{M} \delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M}\mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M} \delta\mathbf{x}_i.
\end{align}&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;\begin{align}
\delta \mathbf{K} \mathbf{x}_{0i} + \mathbf{K}_0\delta \mathbf{x}_i + \delta \mathbf{K}\delta \mathbf{x}_i = \lambda_{0i}\mathbf{M}_0\delta\mathbf{x}_i + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\mathbf{x}_{0i} + \lambda_{0i} \delta \mathbf{M} \delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\delta\mathbf{x}_i +  \delta\lambda_i \delta \mathbf{M} \delta\mathbf{x}_i.
\end{align}&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;\mathbf{K}_0 \delta\mathbf{x}_i+ \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i}\mathbf{M}_0 \delta \mathbf{x}_i + \lambda_{0i}\delta \mathbf{M} \mathrm{x}_{0i} + \delta \lambda_i \mathbf{M}_0\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} \qquad (4)&lt;/math&gt;

where the {{math|''ε&lt;sub&gt;ij&lt;/sub&gt;''}} are small constants that are to be determined. Substituting (4) into (3) and rearranging gives

:&lt;math&gt;\begin{align}
\mathbf{K}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0\mathbf{x}_{0i} &amp;&amp; (5) \\
\sum_{j=1}^N \varepsilon_{ij} \mathbf{K}_0 \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0 \mathbf{x}_{0i} &amp;&amp; \text{Applying } \mathbf{K}_0 \text{ to the sum} \\
\sum_{j=1}^N \varepsilon_{ij} \lambda_{0j} \mathbf{M}_0 \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0 \mathbf{x}_{0i} &amp;&amp; \text{Using Eq. } (1) 
\end{align}&lt;/math&gt;

Because the eigenvectors are {{math|'''M'''&lt;sub&gt;0&lt;/sub&gt;}}-orthogonal when {{math|'''M'''&lt;sub&gt;0&lt;/sub&gt;}} is positive definite, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \varepsilon_{ii} \lambda_{0i} \mathbf{M}_0 \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \mathbf{M}_0 \varepsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top \mathbf{K}_0 \varepsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \mathbf{M}_0\varepsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top \delta \mathbf{M}\mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}. \qquad (6) &lt;/math&gt;

The two terms containing {{math|''ε&lt;sub&gt;ii&lt;/sub&gt;''}} are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top\mathbf{K}_0\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top \mathbf{M}_0\mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i} \left (\delta \mathbf{K}- \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top\mathbf{M}_0 \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}.&lt;/math&gt;

Then, by left-multiplying equation (5) by {{math|'''x'''&lt;sub&gt;0''k''&lt;/sub&gt;}}:

:&lt;math&gt;\varepsilon_{ik} = \frac{\mathbf{x}^\top_{0k} \left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right )\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\varepsilon_{ij} = \frac{\mathbf{x}^\top_{0j} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find {{math|''ε&lt;sub&gt;ii&lt;/sub&gt;''}}, use the fact that:

:&lt;math&gt;\mathbf{x}^\top_i \mathbf{M} \mathbf{x}_i = 1&lt;/math&gt;

implies:

:&lt;math&gt;\varepsilon_{ii}=-\tfrac{1}{2}\mathbf{x}^\top_{0i} \delta \mathbf{M} \mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\begin{align}
\lambda_i &amp;= \lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right ) \mathbf{x}_{0i} \\ 
\mathbf{x}_i &amp;= \mathbf{x}_{0i} \left (1 - \tfrac{1}{2} \mathbf{x}^\top_{0i} \delta \mathbf{M} \mathbf{x}_{0i} \right ) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}\left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right ) \mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}} \mathbf{x}_{0j}
\end{align}&lt;/math&gt;

for infinitesimal {{mvar|δK}} and {{mvar|δM}} (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on {{math|''λ&lt;sub&gt;i&lt;/sub&gt;''}} as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing {{math|'''K'''&lt;sub&gt;''k''ℓ&lt;/sub&gt;}} will also change {{math|'''K'''&lt;sub&gt;ℓ''k''&lt;/sub&gt;}}, hence the {{math|(2 − ''δ''&lt;sub&gt;''k''ℓ&lt;/sub&gt;)}} term.)

:&lt;math&gt;\begin{align}
\frac{\partial \lambda_i}{\partial \mathbf{K}_{(k\ell)}} &amp;= \frac{\partial}{\partial \mathbf{K}_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right ) \mathbf{x}_{0i} \right) = x_{0i(k)} x_{0i(\ell)} \left (2 - \delta_{k\ell} \right ) \\
\frac{\partial \lambda_i}{\partial \mathbf{M}_{(k\ell)}} &amp;= \frac{\partial}{\partial \mathbf{M}_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right ) \mathbf{x}_{0i}\right) = \lambda_i x_{0i(k)} x_{0i(\ell)} \left (2- \delta_{k\ell} \right ).
\end{align}&lt;/math&gt;

Similarly

:&lt;math&gt;\begin{align}
\frac{\partial\mathbf{x}_i}{\partial \mathbf{K}_{(k\ell)}} &amp;= \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)} \left (2-\delta_{k\ell} \right )}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j} \\
\frac{\partial \mathbf{x}_i}{\partial \mathbf{M}_{(k\ell)}} &amp;= -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_{k\ell}) - \sum_{j=1\atop j\neq i}^N \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j} \left (2-\delta_{k\ell} \right ).
\end{align}&lt;/math&gt;

==Existence of eigenvectors==

Note that in the above example we assumed that both the unperturbed and the perturbed systems involved [[symmetric matrices]], which guaranteed the existence of &lt;math&gt;N&lt;/math&gt; linearly independent eigenvectors. An eigenvalue problem involving non-symmetric matrices is not guaranteed to have &lt;math&gt;N&lt;/math&gt; linearly independent eigenvectors, though a sufficient condition is that &lt;math&gt;\mathbf{K}&lt;/math&gt; and &lt;math&gt;\mathbf{M}&lt;/math&gt; be [[simultaneously diagonalisable]]. 

==See also==
* [[Perturbation theory (quantum mechanics)]]
* [[Bauer–Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=Lloyd N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

== Further reading ==
* {{cite book|editor1-last=Hogben|editor1-first=Leslie|title=Handbook of linear algebra|date=2014|isbn=1466507284|author=Ren-Cang Li|edition=Second edition.|chapter=Matrix Perturbation Theory}}

[[Category:Perturbation theory]]
[[Category:Linear algebra]]
[[Category:Numerical linear algebra]]</text>
			<sha1>rxd6cffakpuo3q9ddaopnn8r37f0t52</sha1>
		</revision>
		<revision>
			<id>656809373</id>
			<parentid>642217981</parentid>
			<timestamp>2015-04-16T21:16:59Z</timestamp>
			<contributor>
				<username>طاها</username>
				<id>16608024</id>
			</contributor>
			<comment>+ {{Disputed}}</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="11774">{{main|Perturbation theory}}
{{Multiple issues|
{{refimprove|date=April 2007}}
{{expert-subject|date=November 2008}}
{{Disputed}}
}}

In mathematics, an '''eigenvalue perturbation''' problem is that of finding the [[eigenvector|eigenvectors and eigenvalues]] of a system that is [[perturbation theory|perturbed]] from one with known eigenvectors and eigenvalues. This is useful for studying how sensitive the original system's eigenvectors and eigenvalues are to changes in the system. 
This type of analysis popularized by [[John William Strutt, 3rd Baron Rayleigh|Lord Rayleigh]], in his investigation of harmonic vibrations of a string perturbed by small inhomogeneities.&lt;ref&gt;{{cite book|first=J. W. S.|last=Rayleigh|title=Theory of Sound|edition=2nd |volume= I|pages=115–118|publisher= Macmillan|location= London |year=1894|isbn=1-152-06023-6}}&lt;/ref&gt; 

The derivations in this article are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. | authorlink=Lloyd N. Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt; or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;\mathbf{K}_0 \mathbf{x}_{0i} = \lambda_{0i} \mathbf{M}_0 \mathbf{x}_{0i}. \qquad (0)&lt;/math&gt;

where &lt;math&gt;\mathbf{K}&lt;/math&gt; and &lt;math&gt;\mathbf{M}&lt;/math&gt; are matrices. That is, we know the eigenvalues {{math|''λ''&lt;sub&gt;0''i''&lt;/sub&gt;}} and eigenvectors {{math|'''x'''&lt;sub&gt;0''i''&lt;/sub&gt;}} for {{math|''i'' {{=}} 1, ..., ''N''}}. Now suppose we want to change the matrices by a small amount. That is, we want to find the eigenvalues and eigenvectors of
:&lt;math&gt;\mathbf{K} \mathbf{x}_i = \lambda_i \mathbf{M} \mathbf{x}_i \qquad (1)&lt;/math&gt;
where
:&lt;math&gt;\begin{align}
\mathbf{K} &amp;= \mathbf{K}_0 + \delta \mathbf{K}\\
\mathbf{M} &amp;= \mathbf{M}_0 + \delta \mathbf{M}
\end{align}&lt;/math&gt;

with the perturbations &lt;math&gt;\delta\mathbf{K}&lt;/math&gt; and &lt;math&gt;\delta\mathbf{M}&lt;/math&gt; much smaller than &lt;math&gt;\mathbf{K}&lt;/math&gt; and &lt;math&gt;\mathbf{M}&lt;/math&gt; respectively. Then we expect the new eigenvalues and eigenvectors to be similar to the original, plus small perturbations:

:&lt;math&gt;\begin{align}
\lambda_i &amp;= \lambda_{0i}+\delta\lambda_{i} \\
\mathbf{x}_i &amp;= \mathbf{x}_{0i} + \delta\mathbf{x}_{i} 
\end{align}&lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]], and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top \mathbf{M}_0\mathbf{x}_{0i} = \delta_{ij} \qquad(2)&lt;/math&gt;

where {{math|''δ&lt;sub&gt;ij&lt;/sub&gt;''}} is the [[Kronecker delta]]. 
Now we want to solve the equation

:&lt;math&gt;\mathbf{K}\mathbf{x}_i = \lambda_i \mathbf{M} \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;(\mathbf{K}_0+\delta \mathbf{K})(\mathbf{x}_{0i} + \delta \mathbf{x}_{i}) = \left (\lambda_{0i}+\delta\lambda_{i} \right ) \left (\mathbf{M}_0+ \delta \mathbf{M} \right ) \left (\mathbf{x}_{0i}+\delta\mathbf{x}_{i} \right ),&lt;/math&gt;

which expands to

:&lt;math&gt;\begin{align}
\mathbf{K}_0\mathbf{x}_{0i} &amp;+ \delta \mathbf{K}\mathbf{x}_{0i} + \mathbf{K}_0\delta \mathbf{x}_i + \delta \mathbf{K}\delta \mathbf{x}_i = \\[6pt]
&amp;=\lambda_{0i}\mathbf{M}_0\mathbf{x}_{0i}+\lambda_{0i}\mathbf{M}_0\delta\mathbf{x}_i + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} +\delta\lambda_i\mathbf{M}_0\mathbf{x}_{0i} + \lambda_{0i} \delta \mathbf{M} \delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M}\mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M} \delta\mathbf{x}_i.
\end{align}&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;\begin{align}
\delta \mathbf{K} \mathbf{x}_{0i} + \mathbf{K}_0\delta \mathbf{x}_i + \delta \mathbf{K}\delta \mathbf{x}_i = \lambda_{0i}\mathbf{M}_0\delta\mathbf{x}_i + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\mathbf{x}_{0i} + \lambda_{0i} \delta \mathbf{M} \delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\delta\mathbf{x}_i +  \delta\lambda_i \delta \mathbf{M} \delta\mathbf{x}_i.
\end{align}&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;\mathbf{K}_0 \delta\mathbf{x}_i+ \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i}\mathbf{M}_0 \delta \mathbf{x}_i + \lambda_{0i}\delta \mathbf{M} \mathrm{x}_{0i} + \delta \lambda_i \mathbf{M}_0\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} \qquad (4)&lt;/math&gt;

where the {{math|''ε&lt;sub&gt;ij&lt;/sub&gt;''}} are small constants that are to be determined. Substituting (4) into (3) and rearranging gives

:&lt;math&gt;\begin{align}
\mathbf{K}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0\mathbf{x}_{0i} &amp;&amp; (5) \\
\sum_{j=1}^N \varepsilon_{ij} \mathbf{K}_0 \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0 \mathbf{x}_{0i} &amp;&amp; \text{Applying } \mathbf{K}_0 \text{ to the sum} \\
\sum_{j=1}^N \varepsilon_{ij} \lambda_{0j} \mathbf{M}_0 \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0 \mathbf{x}_{0i} &amp;&amp; \text{Using Eq. } (1) 
\end{align}&lt;/math&gt;

Because the eigenvectors are {{math|'''M'''&lt;sub&gt;0&lt;/sub&gt;}}-orthogonal when {{math|'''M'''&lt;sub&gt;0&lt;/sub&gt;}} is positive definite, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \varepsilon_{ii} \lambda_{0i} \mathbf{M}_0 \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \mathbf{M}_0 \varepsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top \mathbf{K}_0 \varepsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \mathbf{M}_0\varepsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top \delta \mathbf{M}\mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}. \qquad (6) &lt;/math&gt;

The two terms containing {{math|''ε&lt;sub&gt;ii&lt;/sub&gt;''}} are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top\mathbf{K}_0\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top \mathbf{M}_0\mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i} \left (\delta \mathbf{K}- \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top\mathbf{M}_0 \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}.&lt;/math&gt;

Then, by left-multiplying equation (5) by {{math|'''x'''&lt;sub&gt;0''k''&lt;/sub&gt;}}:

:&lt;math&gt;\varepsilon_{ik} = \frac{\mathbf{x}^\top_{0k} \left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right )\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\varepsilon_{ij} = \frac{\mathbf{x}^\top_{0j} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find {{math|''ε&lt;sub&gt;ii&lt;/sub&gt;''}}, use the fact that:

:&lt;math&gt;\mathbf{x}^\top_i \mathbf{M} \mathbf{x}_i = 1&lt;/math&gt;

implies:

:&lt;math&gt;\varepsilon_{ii}=-\tfrac{1}{2}\mathbf{x}^\top_{0i} \delta \mathbf{M} \mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\begin{align}
\lambda_i &amp;= \lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right ) \mathbf{x}_{0i} \\ 
\mathbf{x}_i &amp;= \mathbf{x}_{0i} \left (1 - \tfrac{1}{2} \mathbf{x}^\top_{0i} \delta \mathbf{M} \mathbf{x}_{0i} \right ) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}\left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right ) \mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}} \mathbf{x}_{0j}
\end{align}&lt;/math&gt;

for infinitesimal {{mvar|δK}} and {{mvar|δM}} (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on {{math|''λ&lt;sub&gt;i&lt;/sub&gt;''}} as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing {{math|'''K'''&lt;sub&gt;''k''ℓ&lt;/sub&gt;}} will also change {{math|'''K'''&lt;sub&gt;ℓ''k''&lt;/sub&gt;}}, hence the {{math|(2 − ''δ''&lt;sub&gt;''k''ℓ&lt;/sub&gt;)}} term.)

:&lt;math&gt;\begin{align}
\frac{\partial \lambda_i}{\partial \mathbf{K}_{(k\ell)}} &amp;= \frac{\partial}{\partial \mathbf{K}_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right ) \mathbf{x}_{0i} \right) = x_{0i(k)} x_{0i(\ell)} \left (2 - \delta_{k\ell} \right ) \\
\frac{\partial \lambda_i}{\partial \mathbf{M}_{(k\ell)}} &amp;= \frac{\partial}{\partial \mathbf{M}_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right ) \mathbf{x}_{0i}\right) = \lambda_i x_{0i(k)} x_{0i(\ell)} \left (2- \delta_{k\ell} \right ).
\end{align}&lt;/math&gt;

Similarly

:&lt;math&gt;\begin{align}
\frac{\partial\mathbf{x}_i}{\partial \mathbf{K}_{(k\ell)}} &amp;= \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)} \left (2-\delta_{k\ell} \right )}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j} \\
\frac{\partial \mathbf{x}_i}{\partial \mathbf{M}_{(k\ell)}} &amp;= -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_{k\ell}) - \sum_{j=1\atop j\neq i}^N \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j} \left (2-\delta_{k\ell} \right ).
\end{align}&lt;/math&gt;

==Existence of eigenvectors==

Note that in the above example we assumed that both the unperturbed and the perturbed systems involved [[symmetric matrices]], which guaranteed the existence of &lt;math&gt;N&lt;/math&gt; linearly independent eigenvectors. An eigenvalue problem involving non-symmetric matrices is not guaranteed to have &lt;math&gt;N&lt;/math&gt; linearly independent eigenvectors, though a sufficient condition is that &lt;math&gt;\mathbf{K}&lt;/math&gt; and &lt;math&gt;\mathbf{M}&lt;/math&gt; be [[simultaneously diagonalisable]]. 

==See also==
* [[Perturbation theory (quantum mechanics)]]
* [[Bauer–Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=Lloyd N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

== Further reading ==
* {{cite book|editor1-last=Hogben|editor1-first=Leslie|title=Handbook of linear algebra|date=2014|isbn=1466507284|author=Ren-Cang Li|edition=Second edition.|chapter=Matrix Perturbation Theory}}

[[Category:Perturbation theory]]
[[Category:Linear algebra]]
[[Category:Numerical linear algebra]]</text>
			<sha1>pfmjazppvqpj6ri16uekggo31zw6v2j</sha1>
		</revision>
		<revision>
			<id>656821563</id>
			<parentid>656809373</parentid>
			<timestamp>2015-04-16T23:17:24Z</timestamp>
			<contributor>
				<username>AnomieBOT</username>
				<id>7611264</id>
			</contributor>
			<minor />
			<comment>Dating maintenance tags: {{Disputed}}</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="11790">{{main|Perturbation theory}}
{{Multiple issues|
{{refimprove|date=April 2007}}
{{expert-subject|date=November 2008}}
{{Disputed|date=April 2015}}
}}

In mathematics, an '''eigenvalue perturbation''' problem is that of finding the [[eigenvector|eigenvectors and eigenvalues]] of a system that is [[perturbation theory|perturbed]] from one with known eigenvectors and eigenvalues. This is useful for studying how sensitive the original system's eigenvectors and eigenvalues are to changes in the system. 
This type of analysis popularized by [[John William Strutt, 3rd Baron Rayleigh|Lord Rayleigh]], in his investigation of harmonic vibrations of a string perturbed by small inhomogeneities.&lt;ref&gt;{{cite book|first=J. W. S.|last=Rayleigh|title=Theory of Sound|edition=2nd |volume= I|pages=115–118|publisher= Macmillan|location= London |year=1894|isbn=1-152-06023-6}}&lt;/ref&gt; 

The derivations in this article are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. | authorlink=Lloyd N. Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt; or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;\mathbf{K}_0 \mathbf{x}_{0i} = \lambda_{0i} \mathbf{M}_0 \mathbf{x}_{0i}. \qquad (0)&lt;/math&gt;

where &lt;math&gt;\mathbf{K}&lt;/math&gt; and &lt;math&gt;\mathbf{M}&lt;/math&gt; are matrices. That is, we know the eigenvalues {{math|''λ''&lt;sub&gt;0''i''&lt;/sub&gt;}} and eigenvectors {{math|'''x'''&lt;sub&gt;0''i''&lt;/sub&gt;}} for {{math|''i'' {{=}} 1, ..., ''N''}}. Now suppose we want to change the matrices by a small amount. That is, we want to find the eigenvalues and eigenvectors of
:&lt;math&gt;\mathbf{K} \mathbf{x}_i = \lambda_i \mathbf{M} \mathbf{x}_i \qquad (1)&lt;/math&gt;
where
:&lt;math&gt;\begin{align}
\mathbf{K} &amp;= \mathbf{K}_0 + \delta \mathbf{K}\\
\mathbf{M} &amp;= \mathbf{M}_0 + \delta \mathbf{M}
\end{align}&lt;/math&gt;

with the perturbations &lt;math&gt;\delta\mathbf{K}&lt;/math&gt; and &lt;math&gt;\delta\mathbf{M}&lt;/math&gt; much smaller than &lt;math&gt;\mathbf{K}&lt;/math&gt; and &lt;math&gt;\mathbf{M}&lt;/math&gt; respectively. Then we expect the new eigenvalues and eigenvectors to be similar to the original, plus small perturbations:

:&lt;math&gt;\begin{align}
\lambda_i &amp;= \lambda_{0i}+\delta\lambda_{i} \\
\mathbf{x}_i &amp;= \mathbf{x}_{0i} + \delta\mathbf{x}_{i} 
\end{align}&lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]], and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top \mathbf{M}_0\mathbf{x}_{0i} = \delta_{ij} \qquad(2)&lt;/math&gt;

where {{math|''δ&lt;sub&gt;ij&lt;/sub&gt;''}} is the [[Kronecker delta]]. 
Now we want to solve the equation

:&lt;math&gt;\mathbf{K}\mathbf{x}_i = \lambda_i \mathbf{M} \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;(\mathbf{K}_0+\delta \mathbf{K})(\mathbf{x}_{0i} + \delta \mathbf{x}_{i}) = \left (\lambda_{0i}+\delta\lambda_{i} \right ) \left (\mathbf{M}_0+ \delta \mathbf{M} \right ) \left (\mathbf{x}_{0i}+\delta\mathbf{x}_{i} \right ),&lt;/math&gt;

which expands to

:&lt;math&gt;\begin{align}
\mathbf{K}_0\mathbf{x}_{0i} &amp;+ \delta \mathbf{K}\mathbf{x}_{0i} + \mathbf{K}_0\delta \mathbf{x}_i + \delta \mathbf{K}\delta \mathbf{x}_i = \\[6pt]
&amp;=\lambda_{0i}\mathbf{M}_0\mathbf{x}_{0i}+\lambda_{0i}\mathbf{M}_0\delta\mathbf{x}_i + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} +\delta\lambda_i\mathbf{M}_0\mathbf{x}_{0i} + \lambda_{0i} \delta \mathbf{M} \delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M}\mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M} \delta\mathbf{x}_i.
\end{align}&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;\begin{align}
\delta \mathbf{K} \mathbf{x}_{0i} + \mathbf{K}_0\delta \mathbf{x}_i + \delta \mathbf{K}\delta \mathbf{x}_i = \lambda_{0i}\mathbf{M}_0\delta\mathbf{x}_i + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\mathbf{x}_{0i} + \lambda_{0i} \delta \mathbf{M} \delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\delta\mathbf{x}_i +  \delta\lambda_i \delta \mathbf{M} \delta\mathbf{x}_i.
\end{align}&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;\mathbf{K}_0 \delta\mathbf{x}_i+ \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i}\mathbf{M}_0 \delta \mathbf{x}_i + \lambda_{0i}\delta \mathbf{M} \mathrm{x}_{0i} + \delta \lambda_i \mathbf{M}_0\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} \qquad (4)&lt;/math&gt;

where the {{math|''ε&lt;sub&gt;ij&lt;/sub&gt;''}} are small constants that are to be determined. Substituting (4) into (3) and rearranging gives

:&lt;math&gt;\begin{align}
\mathbf{K}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0\mathbf{x}_{0i} &amp;&amp; (5) \\
\sum_{j=1}^N \varepsilon_{ij} \mathbf{K}_0 \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0 \mathbf{x}_{0i} &amp;&amp; \text{Applying } \mathbf{K}_0 \text{ to the sum} \\
\sum_{j=1}^N \varepsilon_{ij} \lambda_{0j} \mathbf{M}_0 \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0 \mathbf{x}_{0i} &amp;&amp; \text{Using Eq. } (1) 
\end{align}&lt;/math&gt;

Because the eigenvectors are {{math|'''M'''&lt;sub&gt;0&lt;/sub&gt;}}-orthogonal when {{math|'''M'''&lt;sub&gt;0&lt;/sub&gt;}} is positive definite, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \varepsilon_{ii} \lambda_{0i} \mathbf{M}_0 \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \mathbf{M}_0 \varepsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top \mathbf{K}_0 \varepsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \mathbf{M}_0\varepsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top \delta \mathbf{M}\mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}. \qquad (6) &lt;/math&gt;

The two terms containing {{math|''ε&lt;sub&gt;ii&lt;/sub&gt;''}} are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top\mathbf{K}_0\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top \mathbf{M}_0\mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i} \left (\delta \mathbf{K}- \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top\mathbf{M}_0 \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}.&lt;/math&gt;

Then, by left-multiplying equation (5) by {{math|'''x'''&lt;sub&gt;0''k''&lt;/sub&gt;}}:

:&lt;math&gt;\varepsilon_{ik} = \frac{\mathbf{x}^\top_{0k} \left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right )\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\varepsilon_{ij} = \frac{\mathbf{x}^\top_{0j} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find {{math|''ε&lt;sub&gt;ii&lt;/sub&gt;''}}, use the fact that:

:&lt;math&gt;\mathbf{x}^\top_i \mathbf{M} \mathbf{x}_i = 1&lt;/math&gt;

implies:

:&lt;math&gt;\varepsilon_{ii}=-\tfrac{1}{2}\mathbf{x}^\top_{0i} \delta \mathbf{M} \mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\begin{align}
\lambda_i &amp;= \lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right ) \mathbf{x}_{0i} \\ 
\mathbf{x}_i &amp;= \mathbf{x}_{0i} \left (1 - \tfrac{1}{2} \mathbf{x}^\top_{0i} \delta \mathbf{M} \mathbf{x}_{0i} \right ) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}\left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right ) \mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}} \mathbf{x}_{0j}
\end{align}&lt;/math&gt;

for infinitesimal {{mvar|δK}} and {{mvar|δM}} (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on {{math|''λ&lt;sub&gt;i&lt;/sub&gt;''}} as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing {{math|'''K'''&lt;sub&gt;''k''ℓ&lt;/sub&gt;}} will also change {{math|'''K'''&lt;sub&gt;ℓ''k''&lt;/sub&gt;}}, hence the {{math|(2 − ''δ''&lt;sub&gt;''k''ℓ&lt;/sub&gt;)}} term.)

:&lt;math&gt;\begin{align}
\frac{\partial \lambda_i}{\partial \mathbf{K}_{(k\ell)}} &amp;= \frac{\partial}{\partial \mathbf{K}_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right ) \mathbf{x}_{0i} \right) = x_{0i(k)} x_{0i(\ell)} \left (2 - \delta_{k\ell} \right ) \\
\frac{\partial \lambda_i}{\partial \mathbf{M}_{(k\ell)}} &amp;= \frac{\partial}{\partial \mathbf{M}_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right ) \mathbf{x}_{0i}\right) = \lambda_i x_{0i(k)} x_{0i(\ell)} \left (2- \delta_{k\ell} \right ).
\end{align}&lt;/math&gt;

Similarly

:&lt;math&gt;\begin{align}
\frac{\partial\mathbf{x}_i}{\partial \mathbf{K}_{(k\ell)}} &amp;= \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)} \left (2-\delta_{k\ell} \right )}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j} \\
\frac{\partial \mathbf{x}_i}{\partial \mathbf{M}_{(k\ell)}} &amp;= -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_{k\ell}) - \sum_{j=1\atop j\neq i}^N \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j} \left (2-\delta_{k\ell} \right ).
\end{align}&lt;/math&gt;

==Existence of eigenvectors==

Note that in the above example we assumed that both the unperturbed and the perturbed systems involved [[symmetric matrices]], which guaranteed the existence of &lt;math&gt;N&lt;/math&gt; linearly independent eigenvectors. An eigenvalue problem involving non-symmetric matrices is not guaranteed to have &lt;math&gt;N&lt;/math&gt; linearly independent eigenvectors, though a sufficient condition is that &lt;math&gt;\mathbf{K}&lt;/math&gt; and &lt;math&gt;\mathbf{M}&lt;/math&gt; be [[simultaneously diagonalisable]]. 

==See also==
* [[Perturbation theory (quantum mechanics)]]
* [[Bauer–Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=Lloyd N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

== Further reading ==
* {{cite book|editor1-last=Hogben|editor1-first=Leslie|title=Handbook of linear algebra|date=2014|isbn=1466507284|author=Ren-Cang Li|edition=Second edition.|chapter=Matrix Perturbation Theory}}

[[Category:Perturbation theory]]
[[Category:Linear algebra]]
[[Category:Numerical linear algebra]]</text>
			<sha1>c37mmfkb84xeicy4tyner10shyiyoeq</sha1>
		</revision>
		<revision>
			<id>668674659</id>
			<parentid>656821563</parentid>
			<timestamp>2015-06-25T21:29:07Z</timestamp>
			<contributor>
				<ip>95.90.218.141</ip>
			</contributor>
			<comment>/* Example */</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="11794">{{main|Perturbation theory}}
{{Multiple issues|
{{refimprove|date=April 2007}}
{{expert-subject|date=November 2008}}
{{Disputed|date=April 2015}}
}}

In mathematics, an '''eigenvalue perturbation''' problem is that of finding the [[eigenvector|eigenvectors and eigenvalues]] of a system that is [[perturbation theory|perturbed]] from one with known eigenvectors and eigenvalues. This is useful for studying how sensitive the original system's eigenvectors and eigenvalues are to changes in the system. 
This type of analysis popularized by [[John William Strutt, 3rd Baron Rayleigh|Lord Rayleigh]], in his investigation of harmonic vibrations of a string perturbed by small inhomogeneities.&lt;ref&gt;{{cite book|first=J. W. S.|last=Rayleigh|title=Theory of Sound|edition=2nd |volume= I|pages=115–118|publisher= Macmillan|location= London |year=1894|isbn=1-152-06023-6}}&lt;/ref&gt; 

The derivations in this article are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. | authorlink=Lloyd N. Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt; or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;\mathbf{K}_0 \mathbf{x}_{0i} = \lambda_{0i} \mathbf{M}_0 \mathbf{x}_{0i}. \qquad (0)&lt;/math&gt;

where &lt;math&gt;\mathbf{K}_0&lt;/math&gt; and &lt;math&gt;\mathbf{M}_0&lt;/math&gt; are matrices. That is, we know the eigenvalues {{math|''λ''&lt;sub&gt;0''i''&lt;/sub&gt;}} and eigenvectors {{math|'''x'''&lt;sub&gt;0''i''&lt;/sub&gt;}} for {{math|''i'' {{=}} 1, ..., ''N''}}. Now suppose we want to change the matrices by a small amount. That is, we want to find the eigenvalues and eigenvectors of
:&lt;math&gt;\mathbf{K} \mathbf{x}_i = \lambda_i \mathbf{M} \mathbf{x}_i \qquad (1)&lt;/math&gt;
where
:&lt;math&gt;\begin{align}
\mathbf{K} &amp;= \mathbf{K}_0 + \delta \mathbf{K}\\
\mathbf{M} &amp;= \mathbf{M}_0 + \delta \mathbf{M}
\end{align}&lt;/math&gt;

with the perturbations &lt;math&gt;\delta\mathbf{K}&lt;/math&gt; and &lt;math&gt;\delta\mathbf{M}&lt;/math&gt; much smaller than &lt;math&gt;\mathbf{K}&lt;/math&gt; and &lt;math&gt;\mathbf{M}&lt;/math&gt; respectively. Then we expect the new eigenvalues and eigenvectors to be similar to the original, plus small perturbations:

:&lt;math&gt;\begin{align}
\lambda_i &amp;= \lambda_{0i}+\delta\lambda_{i} \\
\mathbf{x}_i &amp;= \mathbf{x}_{0i} + \delta\mathbf{x}_{i} 
\end{align}&lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]], and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top \mathbf{M}_0\mathbf{x}_{0i} = \delta_{ij} \qquad(2)&lt;/math&gt;

where {{math|''δ&lt;sub&gt;ij&lt;/sub&gt;''}} is the [[Kronecker delta]]. 
Now we want to solve the equation

:&lt;math&gt;\mathbf{K}\mathbf{x}_i = \lambda_i \mathbf{M} \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;(\mathbf{K}_0+\delta \mathbf{K})(\mathbf{x}_{0i} + \delta \mathbf{x}_{i}) = \left (\lambda_{0i}+\delta\lambda_{i} \right ) \left (\mathbf{M}_0+ \delta \mathbf{M} \right ) \left (\mathbf{x}_{0i}+\delta\mathbf{x}_{i} \right ),&lt;/math&gt;

which expands to

:&lt;math&gt;\begin{align}
\mathbf{K}_0\mathbf{x}_{0i} &amp;+ \delta \mathbf{K}\mathbf{x}_{0i} + \mathbf{K}_0\delta \mathbf{x}_i + \delta \mathbf{K}\delta \mathbf{x}_i = \\[6pt]
&amp;=\lambda_{0i}\mathbf{M}_0\mathbf{x}_{0i}+\lambda_{0i}\mathbf{M}_0\delta\mathbf{x}_i + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} +\delta\lambda_i\mathbf{M}_0\mathbf{x}_{0i} + \lambda_{0i} \delta \mathbf{M} \delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M}\mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M} \delta\mathbf{x}_i.
\end{align}&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;\begin{align}
\delta \mathbf{K} \mathbf{x}_{0i} + \mathbf{K}_0\delta \mathbf{x}_i + \delta \mathbf{K}\delta \mathbf{x}_i = \lambda_{0i}\mathbf{M}_0\delta\mathbf{x}_i + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\mathbf{x}_{0i} + \lambda_{0i} \delta \mathbf{M} \delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\delta\mathbf{x}_i +  \delta\lambda_i \delta \mathbf{M} \delta\mathbf{x}_i.
\end{align}&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;\mathbf{K}_0 \delta\mathbf{x}_i+ \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i}\mathbf{M}_0 \delta \mathbf{x}_i + \lambda_{0i}\delta \mathbf{M} \mathrm{x}_{0i} + \delta \lambda_i \mathbf{M}_0\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} \qquad (4)&lt;/math&gt;

where the {{math|''ε&lt;sub&gt;ij&lt;/sub&gt;''}} are small constants that are to be determined. Substituting (4) into (3) and rearranging gives

:&lt;math&gt;\begin{align}
\mathbf{K}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0\mathbf{x}_{0i} &amp;&amp; (5) \\
\sum_{j=1}^N \varepsilon_{ij} \mathbf{K}_0 \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0 \mathbf{x}_{0i} &amp;&amp; \text{Applying } \mathbf{K}_0 \text{ to the sum} \\
\sum_{j=1}^N \varepsilon_{ij} \lambda_{0j} \mathbf{M}_0 \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0 \mathbf{x}_{0i} &amp;&amp; \text{Using Eq. } (1) 
\end{align}&lt;/math&gt;

Because the eigenvectors are {{math|'''M'''&lt;sub&gt;0&lt;/sub&gt;}}-orthogonal when {{math|'''M'''&lt;sub&gt;0&lt;/sub&gt;}} is positive definite, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \varepsilon_{ii} \lambda_{0i} \mathbf{M}_0 \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \mathbf{M}_0 \varepsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top \mathbf{K}_0 \varepsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \mathbf{M}_0\varepsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top \delta \mathbf{M}\mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}. \qquad (6) &lt;/math&gt;

The two terms containing {{math|''ε&lt;sub&gt;ii&lt;/sub&gt;''}} are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top\mathbf{K}_0\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top \mathbf{M}_0\mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i} \left (\delta \mathbf{K}- \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top\mathbf{M}_0 \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}.&lt;/math&gt;

Then, by left-multiplying equation (5) by {{math|'''x'''&lt;sub&gt;0''k''&lt;/sub&gt;}}:

:&lt;math&gt;\varepsilon_{ik} = \frac{\mathbf{x}^\top_{0k} \left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right )\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\varepsilon_{ij} = \frac{\mathbf{x}^\top_{0j} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find {{math|''ε&lt;sub&gt;ii&lt;/sub&gt;''}}, use the fact that:

:&lt;math&gt;\mathbf{x}^\top_i \mathbf{M} \mathbf{x}_i = 1&lt;/math&gt;

implies:

:&lt;math&gt;\varepsilon_{ii}=-\tfrac{1}{2}\mathbf{x}^\top_{0i} \delta \mathbf{M} \mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\begin{align}
\lambda_i &amp;= \lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right ) \mathbf{x}_{0i} \\ 
\mathbf{x}_i &amp;= \mathbf{x}_{0i} \left (1 - \tfrac{1}{2} \mathbf{x}^\top_{0i} \delta \mathbf{M} \mathbf{x}_{0i} \right ) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}\left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right ) \mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}} \mathbf{x}_{0j}
\end{align}&lt;/math&gt;

for infinitesimal {{mvar|δK}} and {{mvar|δM}} (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on {{math|''λ&lt;sub&gt;i&lt;/sub&gt;''}} as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing {{math|'''K'''&lt;sub&gt;''k''ℓ&lt;/sub&gt;}} will also change {{math|'''K'''&lt;sub&gt;ℓ''k''&lt;/sub&gt;}}, hence the {{math|(2 − ''δ''&lt;sub&gt;''k''ℓ&lt;/sub&gt;)}} term.)

:&lt;math&gt;\begin{align}
\frac{\partial \lambda_i}{\partial \mathbf{K}_{(k\ell)}} &amp;= \frac{\partial}{\partial \mathbf{K}_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right ) \mathbf{x}_{0i} \right) = x_{0i(k)} x_{0i(\ell)} \left (2 - \delta_{k\ell} \right ) \\
\frac{\partial \lambda_i}{\partial \mathbf{M}_{(k\ell)}} &amp;= \frac{\partial}{\partial \mathbf{M}_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right ) \mathbf{x}_{0i}\right) = \lambda_i x_{0i(k)} x_{0i(\ell)} \left (2- \delta_{k\ell} \right ).
\end{align}&lt;/math&gt;

Similarly

:&lt;math&gt;\begin{align}
\frac{\partial\mathbf{x}_i}{\partial \mathbf{K}_{(k\ell)}} &amp;= \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)} \left (2-\delta_{k\ell} \right )}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j} \\
\frac{\partial \mathbf{x}_i}{\partial \mathbf{M}_{(k\ell)}} &amp;= -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_{k\ell}) - \sum_{j=1\atop j\neq i}^N \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j} \left (2-\delta_{k\ell} \right ).
\end{align}&lt;/math&gt;

==Existence of eigenvectors==

Note that in the above example we assumed that both the unperturbed and the perturbed systems involved [[symmetric matrices]], which guaranteed the existence of &lt;math&gt;N&lt;/math&gt; linearly independent eigenvectors. An eigenvalue problem involving non-symmetric matrices is not guaranteed to have &lt;math&gt;N&lt;/math&gt; linearly independent eigenvectors, though a sufficient condition is that &lt;math&gt;\mathbf{K}&lt;/math&gt; and &lt;math&gt;\mathbf{M}&lt;/math&gt; be [[simultaneously diagonalisable]]. 

==See also==
* [[Perturbation theory (quantum mechanics)]]
* [[Bauer–Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=Lloyd N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

== Further reading ==
* {{cite book|editor1-last=Hogben|editor1-first=Leslie|title=Handbook of linear algebra|date=2014|isbn=1466507284|author=Ren-Cang Li|edition=Second edition.|chapter=Matrix Perturbation Theory}}

[[Category:Perturbation theory]]
[[Category:Linear algebra]]
[[Category:Numerical linear algebra]]</text>
			<sha1>qmk4nx02knlnn53yf4eu7qkp1hzjw9k</sha1>
		</revision>
		<revision>
			<id>686532391</id>
			<parentid>668674659</parentid>
			<timestamp>2015-10-19T18:19:49Z</timestamp>
			<contributor>
				<ip>128.237.190.200</ip>
			</contributor>
			<comment>missing &quot;was&quot;</comment>
			<model>wikitext</model>
			<format>text/x-wiki</format>
			<text xml:space="preserve" bytes="11798">{{main|Perturbation theory}}
{{Multiple issues|
{{refimprove|date=April 2007}}
{{expert-subject|date=November 2008}}
{{Disputed|date=April 2015}}
}}

In mathematics, an '''eigenvalue perturbation''' problem is that of finding the [[eigenvector|eigenvectors and eigenvalues]] of a system that is [[perturbation theory|perturbed]] from one with known eigenvectors and eigenvalues. This is useful for studying how sensitive the original system's eigenvectors and eigenvalues are to changes in the system. 
This type of analysis was popularized by [[John William Strutt, 3rd Baron Rayleigh|Lord Rayleigh]], in his investigation of harmonic vibrations of a string perturbed by small inhomogeneities.&lt;ref&gt;{{cite book|first=J. W. S.|last=Rayleigh|title=Theory of Sound|edition=2nd |volume= I|pages=115–118|publisher= Macmillan|location= London |year=1894|isbn=1-152-06023-6}}&lt;/ref&gt; 

The derivations in this article are essentially self-contained and can be found in many texts on numerical linear algebra&lt;ref name=&quot;Trefethen258&quot;&gt;{{cite book |title=Numerical Linear Algebra |last=Trefethen |first=Lloyd N. | authorlink=Lloyd N. Trefethen|year=1997 |publisher=SIAM (Philadelphia, PA) |isbn=0-89871-361-7 |page=258 }}&lt;/ref&gt; or numerical functional analysis.

==Example==
Suppose we have solutions to the [[generalized eigenvalue problem]],

:&lt;math&gt;\mathbf{K}_0 \mathbf{x}_{0i} = \lambda_{0i} \mathbf{M}_0 \mathbf{x}_{0i}. \qquad (0)&lt;/math&gt;

where &lt;math&gt;\mathbf{K}_0&lt;/math&gt; and &lt;math&gt;\mathbf{M}_0&lt;/math&gt; are matrices. That is, we know the eigenvalues {{math|''λ''&lt;sub&gt;0''i''&lt;/sub&gt;}} and eigenvectors {{math|'''x'''&lt;sub&gt;0''i''&lt;/sub&gt;}} for {{math|''i'' {{=}} 1, ..., ''N''}}. Now suppose we want to change the matrices by a small amount. That is, we want to find the eigenvalues and eigenvectors of
:&lt;math&gt;\mathbf{K} \mathbf{x}_i = \lambda_i \mathbf{M} \mathbf{x}_i \qquad (1)&lt;/math&gt;
where
:&lt;math&gt;\begin{align}
\mathbf{K} &amp;= \mathbf{K}_0 + \delta \mathbf{K}\\
\mathbf{M} &amp;= \mathbf{M}_0 + \delta \mathbf{M}
\end{align}&lt;/math&gt;

with the perturbations &lt;math&gt;\delta\mathbf{K}&lt;/math&gt; and &lt;math&gt;\delta\mathbf{M}&lt;/math&gt; much smaller than &lt;math&gt;\mathbf{K}&lt;/math&gt; and &lt;math&gt;\mathbf{M}&lt;/math&gt; respectively. Then we expect the new eigenvalues and eigenvectors to be similar to the original, plus small perturbations:

:&lt;math&gt;\begin{align}
\lambda_i &amp;= \lambda_{0i}+\delta\lambda_{i} \\
\mathbf{x}_i &amp;= \mathbf{x}_{0i} + \delta\mathbf{x}_{i} 
\end{align}&lt;/math&gt;

==Steps==
We assume that the matrices are [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]], and assume we have scaled the eigenvectors such that

: &lt;math&gt;\mathbf{x}_{0j}^\top \mathbf{M}_0\mathbf{x}_{0i} = \delta_{ij} \qquad(2)&lt;/math&gt;

where {{math|''δ&lt;sub&gt;ij&lt;/sub&gt;''}} is the [[Kronecker delta]]. 
Now we want to solve the equation

:&lt;math&gt;\mathbf{K}\mathbf{x}_i = \lambda_i \mathbf{M} \mathbf{x}_i. &lt;/math&gt;

Substituting, we get

:&lt;math&gt;(\mathbf{K}_0+\delta \mathbf{K})(\mathbf{x}_{0i} + \delta \mathbf{x}_{i}) = \left (\lambda_{0i}+\delta\lambda_{i} \right ) \left (\mathbf{M}_0+ \delta \mathbf{M} \right ) \left (\mathbf{x}_{0i}+\delta\mathbf{x}_{i} \right ),&lt;/math&gt;

which expands to

:&lt;math&gt;\begin{align}
\mathbf{K}_0\mathbf{x}_{0i} &amp;+ \delta \mathbf{K}\mathbf{x}_{0i} + \mathbf{K}_0\delta \mathbf{x}_i + \delta \mathbf{K}\delta \mathbf{x}_i = \\[6pt]
&amp;=\lambda_{0i}\mathbf{M}_0\mathbf{x}_{0i}+\lambda_{0i}\mathbf{M}_0\delta\mathbf{x}_i + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} +\delta\lambda_i\mathbf{M}_0\mathbf{x}_{0i} + \lambda_{0i} \delta \mathbf{M} \delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M}\mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M} \delta\mathbf{x}_i.
\end{align}&lt;/math&gt;

Canceling from (1) leaves

:&lt;math&gt;\begin{align}
\delta \mathbf{K} \mathbf{x}_{0i} + \mathbf{K}_0\delta \mathbf{x}_i + \delta \mathbf{K}\delta \mathbf{x}_i = \lambda_{0i}\mathbf{M}_0\delta\mathbf{x}_i + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\mathbf{x}_{0i} + \lambda_{0i} \delta \mathbf{M} \delta\mathbf{x}_i + \delta\lambda_i \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{M}_0\delta\mathbf{x}_i +  \delta\lambda_i \delta \mathbf{M} \delta\mathbf{x}_i.
\end{align}&lt;/math&gt;

Removing the higher-order terms, this simplifies to

:&lt;math&gt;\mathbf{K}_0 \delta\mathbf{x}_i+ \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i}\mathbf{M}_0 \delta \mathbf{x}_i + \lambda_{0i}\delta \mathbf{M} \mathrm{x}_{0i} + \delta \lambda_i \mathbf{M}_0\mathbf{x}_{0i}. \qquad(3)&lt;/math&gt;

When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct

:&lt;math&gt;\delta \mathbf{x}_i = \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} \qquad (4)&lt;/math&gt;

where the {{math|''ε&lt;sub&gt;ij&lt;/sub&gt;''}} are small constants that are to be determined. Substituting (4) into (3) and rearranging gives

:&lt;math&gt;\begin{align}
\mathbf{K}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0\mathbf{x}_{0i} &amp;&amp; (5) \\
\sum_{j=1}^N \varepsilon_{ij} \mathbf{K}_0 \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0 \mathbf{x}_{0i} &amp;&amp; \text{Applying } \mathbf{K}_0 \text{ to the sum} \\
\sum_{j=1}^N \varepsilon_{ij} \lambda_{0j} \mathbf{M}_0 \mathbf{x}_{0j} + \delta \mathbf{K} \mathbf{x}_{0i} &amp;= \lambda_{0i} \mathbf{M}_0 \sum_{j=1}^N \varepsilon_{ij} \mathbf{x}_{0j} + \lambda_{0i} \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{M}_0 \mathbf{x}_{0i} &amp;&amp; \text{Using Eq. } (1) 
\end{align}&lt;/math&gt;

Because the eigenvectors are {{math|'''M'''&lt;sub&gt;0&lt;/sub&gt;}}-orthogonal when {{math|'''M'''&lt;sub&gt;0&lt;/sub&gt;}} is positive definite, we can remove the summations by left multiplying by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt;:

:&lt;math&gt;\mathbf{x}_{0i}^\top \varepsilon_{ii} \lambda_{0i} \mathbf{M}_0 \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \mathbf{M}_0 \varepsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}. &lt;/math&gt;

By use of equation (1) again:

:&lt;math&gt;\mathbf{x}_{0i}^\top \mathbf{K}_0 \varepsilon_{ii} \mathbf{x}_{0i} + \mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \mathbf{M}_0\varepsilon_{ii} \mathbf{x}_{0i} + \lambda_{0i}\mathbf{x}_{0i}^\top \delta \mathbf{M}\mathbf{x}_{0i} + \delta\lambda_i\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}. \qquad (6) &lt;/math&gt;

The two terms containing {{math|''ε&lt;sub&gt;ii&lt;/sub&gt;''}} are equal because left-multiplying (1) by &lt;math&gt;\mathbf{x}_{0i}^\top&lt;/math&gt; gives

:&lt;math&gt;\mathbf{x}_{0i}^\top\mathbf{K}_0\mathbf{x}_{0i} = \lambda_{0i}\mathbf{x}_{0i}^\top \mathbf{M}_0 \mathbf{x}_{0i}.&lt;/math&gt;

Canceling those terms in (6) leaves

:&lt;math&gt;\mathbf{x}_{0i}^\top \delta \mathbf{K} \mathbf{x}_{0i} = \lambda_{0i} \mathbf{x}_{0i}^\top \delta \mathbf{M} \mathbf{x}_{0i} + \delta\lambda_i \mathbf{x}_{0i}^\top \mathbf{M}_0\mathbf{x}_{0i}.&lt;/math&gt;

Rearranging gives

:&lt;math&gt;\delta\lambda_i  = \frac{\mathbf{x}^\top_{0i} \left (\delta \mathbf{K}- \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}}{\mathbf{x}_{0i}^\top\mathbf{M}_0 \mathbf{x}_{0i}}&lt;/math&gt;

But by (2), this denominator is equal to 1. Thus

:&lt;math&gt;\delta\lambda_i  = \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}.&lt;/math&gt;

Then, by left-multiplying equation (5) by {{math|'''x'''&lt;sub&gt;0''k''&lt;/sub&gt;}}:

:&lt;math&gt;\varepsilon_{ik} = \frac{\mathbf{x}^\top_{0k} \left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right )\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}}, \qquad i\neq k.&lt;/math&gt;

Or by changing the name of the indices:

:&lt;math&gt;\varepsilon_{ij} = \frac{\mathbf{x}^\top_{0j} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right )\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}}, \qquad i\neq j.&lt;/math&gt;

To find {{math|''ε&lt;sub&gt;ii&lt;/sub&gt;''}}, use the fact that:

:&lt;math&gt;\mathbf{x}^\top_i \mathbf{M} \mathbf{x}_i = 1&lt;/math&gt;

implies:

:&lt;math&gt;\varepsilon_{ii}=-\tfrac{1}{2}\mathbf{x}^\top_{0i} \delta \mathbf{M} \mathbf{x}_{0i}.&lt;/math&gt;

== Summary ==
:&lt;math&gt;\begin{align}
\lambda_i &amp;= \lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right ) \mathbf{x}_{0i} \\ 
\mathbf{x}_i &amp;= \mathbf{x}_{0i} \left (1 - \tfrac{1}{2} \mathbf{x}^\top_{0i} \delta \mathbf{M} \mathbf{x}_{0i} \right ) + \sum_{j=1\atop j\neq i}^N \frac{\mathbf{x}^\top_{0j}\left (\delta \mathbf{K} - \lambda_{0i}\delta \mathbf{M} \right ) \mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}} \mathbf{x}_{0j}
\end{align}&lt;/math&gt;

for infinitesimal {{mvar|δK}} and {{mvar|δM}} (the high order terms in (3) being negligible)

==Results==
This means it is possible to efficiently do a [[sensitivity analysis]] on {{math|''λ&lt;sub&gt;i&lt;/sub&gt;''}} as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing {{math|'''K'''&lt;sub&gt;''k''ℓ&lt;/sub&gt;}} will also change {{math|'''K'''&lt;sub&gt;ℓ''k''&lt;/sub&gt;}}, hence the {{math|(2 − ''δ''&lt;sub&gt;''k''ℓ&lt;/sub&gt;)}} term.)

:&lt;math&gt;\begin{align}
\frac{\partial \lambda_i}{\partial \mathbf{K}_{(k\ell)}} &amp;= \frac{\partial}{\partial \mathbf{K}_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right ) \mathbf{x}_{0i} \right) = x_{0i(k)} x_{0i(\ell)} \left (2 - \delta_{k\ell} \right ) \\
\frac{\partial \lambda_i}{\partial \mathbf{M}_{(k\ell)}} &amp;= \frac{\partial}{\partial \mathbf{M}_{(k\ell)}}\left(\lambda_{0i} + \mathbf{x}^\top_{0i} \left (\delta \mathbf{K} - \lambda_{0i} \delta \mathbf{M} \right ) \mathbf{x}_{0i}\right) = \lambda_i x_{0i(k)} x_{0i(\ell)} \left (2- \delta_{k\ell} \right ).
\end{align}&lt;/math&gt;

Similarly

:&lt;math&gt;\begin{align}
\frac{\partial\mathbf{x}_i}{\partial \mathbf{K}_{(k\ell)}} &amp;= \sum_{j=1\atop j\neq i}^N \frac{x_{0j(k)} x_{0i(\ell)} \left (2-\delta_{k\ell} \right )}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j} \\
\frac{\partial \mathbf{x}_i}{\partial \mathbf{M}_{(k\ell)}} &amp;= -\mathbf{x}_{0i}\frac{x_{0i(k)}x_{0i(\ell)}}{2}(2-\delta_{k\ell}) - \sum_{j=1\atop j\neq i}^N \frac{\lambda_{0i}x_{0j(k)} x_{0i(\ell)}}{\lambda_{0i}-\lambda_{0j}}\mathbf{x}_{0j} \left (2-\delta_{k\ell} \right ).
\end{align}&lt;/math&gt;

==Existence of eigenvectors==

Note that in the above example we assumed that both the unperturbed and the perturbed systems involved [[symmetric matrices]], which guaranteed the existence of &lt;math&gt;N&lt;/math&gt; linearly independent eigenvectors. An eigenvalue problem involving non-symmetric matrices is not guaranteed to have &lt;math&gt;N&lt;/math&gt; linearly independent eigenvectors, though a sufficient condition is that &lt;math&gt;\mathbf{K}&lt;/math&gt; and &lt;math&gt;\mathbf{M}&lt;/math&gt; be [[simultaneously diagonalisable]]. 

==See also==
* [[Perturbation theory (quantum mechanics)]]
* [[Bauer–Fike theorem]]

==References==
&lt;references&gt;
&lt;!-- Please keep these in alphabetical order. --&gt;

*{{cite book |first=Lloyd N. |last=Trefethen |year=1997 |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia, PA|isbn = 0-89871-361-7}}

&lt;/references&gt;

== Further reading ==
* {{cite book|editor1-last=Hogben|editor1-first=Leslie|title=Handbook of linear algebra|date=2014|isbn=1466507284|author=Ren-Cang Li|edition=Second edition.|chapter=Matrix Perturbation Theory}}

[[Category:Perturbation theory]]
[[Category:Linear algebra]]
[[Category:Numerical linear algebra]]</text>
			<sha1>i0c8gofprkirzwgh6k6hvwzf2tg7log</sha1>
		</revision>
	</page>
</mediawiki>
